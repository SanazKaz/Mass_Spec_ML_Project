{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 6, 6])\n",
      "torch.Size([10000, 2000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "# Load the data ---- these are binned ones\n",
    "Interaction_matrices = load_data('interaction_matrices_10binned.pkl')\n",
    "spectral_data = load_data('spectra_dataset_10binned.pkl')\n",
    "\n",
    "print(Interaction_matrices.shape)\n",
    "print(spectral_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 20000, 1])\n",
      "torch.Size([4000, 36])\n",
      "torch.Size([10000, 36])\n",
      "torch.Size([10000, 20000, 1])\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/fvfmdq_j709g1tthj47t5fcm0000gn/T/ipykernel_34054/1288698792.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  spectral_data_tensor = torch.tensor(spectral_data.unsqueeze(-1), dtype=torch.float32)\n",
      "/var/folders/02/fvfmdq_j709g1tthj47t5fcm0000gn/T/ipykernel_34054/1288698792.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  matrices_tensor = torch.tensor(Interaction_matrices.view(-1, 36), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# reshaping for LSTM - added one dimension at the end\n",
    "spectral_data_tensor = torch.tensor(spectral_data.unsqueeze(-1), dtype=torch.float32)\n",
    "matrices_tensor = torch.tensor(Interaction_matrices.view(-1, 36), dtype=torch.float32)\n",
    "\n",
    "new_tensor = spectral_data_tensor[:4000]\n",
    "new_matrices = matrices_tensor[:4000]\n",
    "print(new_tensor.shape)\n",
    "print(new_matrices.shape)\n",
    "\n",
    "print(matrices_tensor.shape)\n",
    "print(spectral_data_tensor.shape)\n",
    "\n",
    "print(spectral_data.dtype)\n",
    "print(Interaction_matrices.dtype)\n",
    "\n",
    "# already float32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3200, 20000, 1])\n",
      "torch.Size([400, 20000, 1])\n",
      "torch.Size([400, 20000, 1])\n",
      "torch.Size([3200, 36])\n",
      "torch.Size([400, 36])\n",
      "torch.Size([400, 36])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "spec_train, spec_test, matrix_train, matrix_test = train_test_split(new_tensor, new_matrices, test_size=0.2, random_state=42)\n",
    "spec_test, spec_val, matrix_test, matrix_val = train_test_split(spec_test, matrix_test, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# reshaped for LSTM\n",
    "print(spec_train.shape)\n",
    "print(spec_test.shape)\n",
    "print(spec_val.shape)\n",
    "\n",
    "print(matrix_train.shape)\n",
    "print(matrix_test.shape)\n",
    "print(matrix_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "# pytorch dataset and loaders\n",
    "train_dataset = TensorDataset(torch.Tensor(spec_train), torch.Tensor(matrix_train))\n",
    "val_dataset = TensorDataset(torch.Tensor(spec_val), torch.Tensor(matrix_val))\n",
    "test_dataset = TensorDataset(torch.Tensor(spec_test), torch.Tensor(matrix_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20000, 1])\n",
      "torch.Size([64, 36])\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    print(batch_size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=64, num_layers=4, batch_first=True, dropout=0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=128, num_layers=4, batch_first=True, dropout=0.5)\n",
    "        self.lstm3 = nn.LSTM(input_size=128, hidden_size=256, num_layers=4, batch_first=True, dropout=0.5)\n",
    "        self.lstm4 = nn.LSTM(input_size=256, hidden_size=512, num_layers=4, batch_first=True, dropout=0.0)\n",
    "        self.regressor = nn.Linear(512, 36)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward passing\n",
    "        lstm_out, _ = self.lstm1(x)\n",
    "        lstm_out, _ = self.lstm2(lstm_out)\n",
    "        lstm_out, _ = self.lstm3(lstm_out)\n",
    "        lstm_out, _ = self.lstm4(lstm_out)\n",
    "\n",
    "        # only last time step\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # pass output to lstm to fully connected later to predict all 36 values\n",
    "        output = self.regressor(lstm_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = LSTM()\n",
    "\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vj2zgbc1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-field-27</strong> at: <a href='https://wandb.ai/sanaz_team/my-awesome-project/runs/vj2zgbc1' target=\"_blank\">https://wandb.ai/sanaz_team/my-awesome-project/runs/vj2zgbc1</a><br/> View project at: <a href='https://wandb.ai/sanaz_team/my-awesome-project' target=\"_blank\">https://wandb.ai/sanaz_team/my-awesome-project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240426_141948-vj2zgbc1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vj2zgbc1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sanazkazeminia/Documents/Mass_Spec_project/Mass_Spec_ML_Project/wandb/run-20240426_142045-mzcszyvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sanaz_team/my-awesome-project/runs/mzcszyvp' target=\"_blank\">upbeat-spaceship-28</a></strong> to <a href='https://wandb.ai/sanaz_team/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sanaz_team/my-awesome-project' target=\"_blank\">https://wandb.ai/sanaz_team/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sanaz_team/my-awesome-project/runs/mzcszyvp' target=\"_blank\">https://wandb.ai/sanaz_team/my-awesome-project/runs/mzcszyvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Started at Fri Apr 26 14:20:56 2024\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"LSTM\",\n",
    "    \"dataset\": \"4000-part-spectra\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_layers\": 4,\n",
    "    \"hidden_size\": [64, 128, 256, 512],\n",
    "    \"dropout\": [0.5, 0.5, 0.5, 0.0],\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "def train_model(model, data_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Started at {time.ctime()}\")\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for spectra, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectra)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= len(data_loader)\n",
    "        \n",
    "        PATH = 'cp1_new_lstm_structure.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, PATH)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "        print(f\"Ended at {time.ctime()}\\n\")\n",
    "\n",
    "train_model(model, train_loader, criteria, optimizer, epochs=10)\n",
    "\n",
    "# 2m22seconds for 800 dataset epoch 1 batch size 32 total loss ~6.5 lr = 0.001\n",
    "# 2m22seconds for 800 dataset epoch 1 batch size 64 loss ~7.196 lr = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mass_Spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
