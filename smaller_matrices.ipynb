{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([100, 4, 4])\n",
      "torch.Size([100, 2001])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn   \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "Interaction_matrices = load_data('small_matrices.pkl')\n",
    "spectral_data = load_data('small_spectra.pkl')\n",
    "\n",
    "print(Interaction_matrices.shape)\n",
    "print(spectral_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_examples = 5  # Number of examples to plot\n",
    "\n",
    "# Create a figure with appropriate size\n",
    "fig, axs = plt.subplots(nrows=num_examples, ncols=2, figsize=(10, 6))\n",
    "\n",
    "# Iterate over the examples\n",
    "for i in range(num_examples):\n",
    "    # Plot the spectrum\n",
    "    axs[i, 0].plot(spectral_data[i])\n",
    "\n",
    "    # Plot the associated matrix\n",
    "    axs[i, 1].imshow(Interaction_matrices[i], cmap='viridis')\n",
    "    axs[i, 1].set_title('Matrix')\n",
    "    axs[i, 1].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for matrix in Interaction_matrices: # forgot to zero out the diagonal\n",
    "    matrix[0,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# flattening and stacking interaction matrices\n",
    "flattened_matrices = [matrix.flatten() for matrix in Interaction_matrices] # storing in an array\n",
    "flattened_matrices = torch.stack(flattened_matrices) # stacking the array\n",
    "print(flattened_matrices.shape)\n",
    "print(flattened_matrices[0]) # example to see the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spectral_data, flattened_matrices, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "batch_size = 32 ## 512 is a good number for now. ?? 256 also worked well.\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SpectralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectralNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2001, 1024), # 2001 is the number of features in the spectra data\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),  # Added layer\n",
    "            nn.ReLU(),  # Added activation function\n",
    "            nn.Linear(64, 16), # 16 is the number of features in the flattened interaction matrix (multilabel classification)\n",
    "            nn.Sigmoid() # sigmoid activation function for multilabel classification \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "model = SpectralNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) # lr = 0.001 works well\n",
    "scheduler = CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=2000, mode='triangular')\n",
    "\n",
    "#criterion = AsymmetricLossMultiLabel(gamma_neg=4, gamma_pos=0, clip=1.0) \n",
    "criterion = nn.BCELoss() # binary cross entropy loss for multilabel classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the gradient norms\n",
    "def compute_gradient_norms(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, hamming_loss, precision_score, recall_score, jaccard_score\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, criterion, num_epochs):\n",
    "    train_epoch_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            X, y = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            # Accumulate batch loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            gradient_norm = compute_gradient_norms(model)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Append the predictions and true labels for the epoch\n",
    "\n",
    "            binary_predictions = (y_pred > 0.5).float() # hard thresholding\n",
    "            y_pred_list.append(binary_predictions.cpu().numpy())\n",
    "            y_true_list.append(y.cpu().numpy())\n",
    "        \n",
    "    \n",
    "\n",
    "        # Average loss for the epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        train_epoch_losses.append(train_loss)\n",
    "\n",
    "        y_true = np.concatenate(y_true_list, axis=0)\n",
    "        y_pred = np.concatenate(y_pred_list, axis=0)\n",
    "\n",
    "        y_true = y_true.astype(int)\n",
    "        y_pred = y_pred.astype(int)\n",
    "\n",
    "        hamming_loss_value = hamming_loss(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        jaccard_score_value = jaccard_score(y_true, y_pred, average='micro')\n",
    "\n",
    "                    \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],jacquard_loss:{jaccard_score_value}, hammingloss: {hamming_loss_value:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")     \n",
    "\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Gradient Norm\": gradient_norm,\n",
    "            \"Train Hamming Loss\": hamming_loss_value,\n",
    "            \"Train Precision\": precision,\n",
    "            \"Train Recall\": recall,\n",
    "            \"Train F1\": f1,\n",
    "            \"Train Jaccard Score\": jaccard_score_value\n",
    "\n",
    "        })\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "                state = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'loss': train_loss,\n",
    "                }\n",
    "                torch.save(state, \"binary_model.pth\") ## save the model every x epochs\n",
    "                print(\"Saved model to:\", \"binary_model.pth\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "    return train_epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanazkazeminia97\u001b[0m (\u001b[33msanaz_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sanazkazeminia/Documents/Mass_Spec_project/Mass_Spec_ML_Project/wandb/run-20240524_174305-67klxf9b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sanaz_team/4x4_matrices/runs/67klxf9b' target=\"_blank\">electric-fire-3</a></strong> to <a href='https://wandb.ai/sanaz_team/4x4_matrices' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sanaz_team/4x4_matrices' target=\"_blank\">https://wandb.ai/sanaz_team/4x4_matrices</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sanaz_team/4x4_matrices/runs/67klxf9b' target=\"_blank\">https://wandb.ai/sanaz_team/4x4_matrices/runs/67klxf9b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [1/3000], Train Loss: 0.6903\n",
      "Epoch [2/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [2/3000], Train Loss: 0.6902\n",
      "Epoch [3/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [3/3000], Train Loss: 0.6901\n",
      "Epoch [4/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [4/3000], Train Loss: 0.6900\n",
      "Epoch [5/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [5/3000], Train Loss: 0.6899\n",
      "Epoch [6/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [6/3000], Train Loss: 0.6898\n",
      "Epoch [7/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [7/3000], Train Loss: 0.6897\n",
      "Epoch [8/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [8/3000], Train Loss: 0.6896\n",
      "Epoch [9/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [9/3000], Train Loss: 0.6894\n",
      "Epoch [10/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [10/3000], Train Loss: 0.6893\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [11/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [11/3000], Train Loss: 0.6891\n",
      "Epoch [12/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [12/3000], Train Loss: 0.6890\n",
      "Epoch [13/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [13/3000], Train Loss: 0.6888\n",
      "Epoch [14/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [14/3000], Train Loss: 0.6886\n",
      "Epoch [15/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [15/3000], Train Loss: 0.6884\n",
      "Epoch [16/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [16/3000], Train Loss: 0.6882\n",
      "Epoch [17/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [17/3000], Train Loss: 0.6880\n",
      "Epoch [18/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [18/3000], Train Loss: 0.6878\n",
      "Epoch [19/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [19/3000], Train Loss: 0.6876\n",
      "Epoch [20/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [20/3000], Train Loss: 0.6873\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [21/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [21/3000], Train Loss: 0.6871\n",
      "Epoch [22/3000],jacquard_loss:0.12713178294573643, hammingloss: 0.4398, Precision: 0.1708, Recall: 0.3320, F1: 0.2256\n",
      "Epoch [22/3000], Train Loss: 0.6868\n",
      "Epoch [23/3000],jacquard_loss:0.12225705329153605, hammingloss: 0.4375, Precision: 0.1663, Recall: 0.3158, F1: 0.2179\n",
      "Epoch [23/3000], Train Loss: 0.6865\n",
      "Epoch [24/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [24/3000], Train Loss: 0.6862\n",
      "Epoch [25/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [25/3000], Train Loss: 0.6858\n",
      "Epoch [26/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [26/3000], Train Loss: 0.6855\n",
      "Epoch [27/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [27/3000], Train Loss: 0.6851\n",
      "Epoch [28/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [28/3000], Train Loss: 0.6848\n",
      "Epoch [29/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [29/3000], Train Loss: 0.6843\n",
      "Epoch [30/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [30/3000], Train Loss: 0.6839\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [31/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [31/3000], Train Loss: 0.6834\n",
      "Epoch [32/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [32/3000], Train Loss: 0.6828\n",
      "Epoch [33/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [33/3000], Train Loss: 0.6822\n",
      "Epoch [34/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [34/3000], Train Loss: 0.6816\n",
      "Epoch [35/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [35/3000], Train Loss: 0.6808\n",
      "Epoch [36/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [36/3000], Train Loss: 0.6799\n",
      "Epoch [37/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [37/3000], Train Loss: 0.6790\n",
      "Epoch [38/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [38/3000], Train Loss: 0.6778\n",
      "Epoch [39/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [39/3000], Train Loss: 0.6765\n",
      "Epoch [40/3000],jacquard_loss:0.11551724137931034, hammingloss: 0.4008, Precision: 0.1675, Recall: 0.2713, F1: 0.2071\n",
      "Epoch [40/3000], Train Loss: 0.6750\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [41/3000],jacquard_loss:0.11310592459605028, hammingloss: 0.3859, Precision: 0.1689, Recall: 0.2551, F1: 0.2032\n",
      "Epoch [41/3000], Train Loss: 0.6732\n",
      "Epoch [42/3000],jacquard_loss:0.12072892938496584, hammingloss: 0.3016, Precision: 0.2163, Recall: 0.2146, F1: 0.2154\n",
      "Epoch [42/3000], Train Loss: 0.6711\n",
      "Epoch [43/3000],jacquard_loss:0.12211981566820276, hammingloss: 0.2977, Precision: 0.2208, Recall: 0.2146, F1: 0.2177\n",
      "Epoch [43/3000], Train Loss: 0.6687\n",
      "Epoch [44/3000],jacquard_loss:0.12211981566820276, hammingloss: 0.2977, Precision: 0.2208, Recall: 0.2146, F1: 0.2177\n",
      "Epoch [44/3000], Train Loss: 0.6658\n",
      "Epoch [45/3000],jacquard_loss:0.12211981566820276, hammingloss: 0.2977, Precision: 0.2208, Recall: 0.2146, F1: 0.2177\n",
      "Epoch [45/3000], Train Loss: 0.6623\n",
      "Epoch [46/3000],jacquard_loss:0.12211981566820276, hammingloss: 0.2977, Precision: 0.2208, Recall: 0.2146, F1: 0.2177\n",
      "Epoch [46/3000], Train Loss: 0.6583\n",
      "Epoch [47/3000],jacquard_loss:0.12211981566820276, hammingloss: 0.2977, Precision: 0.2208, Recall: 0.2146, F1: 0.2177\n",
      "Epoch [47/3000], Train Loss: 0.6535\n",
      "Epoch [48/3000],jacquard_loss:0.12037037037037036, hammingloss: 0.2969, Precision: 0.2194, Recall: 0.2105, F1: 0.2149\n",
      "Epoch [48/3000], Train Loss: 0.6480\n",
      "Epoch [49/3000],jacquard_loss:0.10379746835443038, hammingloss: 0.2766, Precision: 0.2169, Recall: 0.1660, F1: 0.1881\n",
      "Epoch [49/3000], Train Loss: 0.6416\n",
      "Epoch [50/3000],jacquard_loss:0.11548556430446194, hammingloss: 0.2633, Precision: 0.2472, Recall: 0.1781, F1: 0.2071\n",
      "Epoch [50/3000], Train Loss: 0.6343\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [51/3000],jacquard_loss:0.11547911547911548, hammingloss: 0.2812, Precision: 0.2271, Recall: 0.1903, F1: 0.2070\n",
      "Epoch [51/3000], Train Loss: 0.6261\n",
      "Epoch [52/3000],jacquard_loss:0.10681818181818181, hammingloss: 0.3070, Precision: 0.1958, Recall: 0.1903, F1: 0.1930\n",
      "Epoch [52/3000], Train Loss: 0.6170\n",
      "Epoch [53/3000],jacquard_loss:0.10681818181818181, hammingloss: 0.3070, Precision: 0.1958, Recall: 0.1903, F1: 0.1930\n",
      "Epoch [53/3000], Train Loss: 0.6070\n",
      "Epoch [54/3000],jacquard_loss:0.1053864168618267, hammingloss: 0.2984, Precision: 0.2000, Recall: 0.1822, F1: 0.1907\n",
      "Epoch [54/3000], Train Loss: 0.5965\n",
      "Epoch [55/3000],jacquard_loss:0.08244680851063829, hammingloss: 0.2695, Precision: 0.1938, Recall: 0.1255, F1: 0.1523\n",
      "Epoch [55/3000], Train Loss: 0.5857\n",
      "Epoch [56/3000],jacquard_loss:0.08244680851063829, hammingloss: 0.2695, Precision: 0.1938, Recall: 0.1255, F1: 0.1523\n",
      "Epoch [56/3000], Train Loss: 0.5750\n",
      "Epoch [57/3000],jacquard_loss:0.08244680851063829, hammingloss: 0.2695, Precision: 0.1938, Recall: 0.1255, F1: 0.1523\n",
      "Epoch [57/3000], Train Loss: 0.5649\n",
      "Epoch [58/3000],jacquard_loss:0.08244680851063829, hammingloss: 0.2695, Precision: 0.1938, Recall: 0.1255, F1: 0.1523\n",
      "Epoch [58/3000], Train Loss: 0.5559\n",
      "Epoch [59/3000],jacquard_loss:0.08244680851063829, hammingloss: 0.2695, Precision: 0.1938, Recall: 0.1255, F1: 0.1523\n",
      "Epoch [59/3000], Train Loss: 0.5481\n",
      "Epoch [60/3000],jacquard_loss:0.07754010695187166, hammingloss: 0.2695, Precision: 0.1859, Recall: 0.1174, F1: 0.1439\n",
      "Epoch [60/3000], Train Loss: 0.5417\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [61/3000],jacquard_loss:0.05144694533762058, hammingloss: 0.2305, Precision: 0.2000, Recall: 0.0648, F1: 0.0979\n",
      "Epoch [61/3000], Train Loss: 0.5362\n",
      "Epoch [62/3000],jacquard_loss:0.05144694533762058, hammingloss: 0.2305, Precision: 0.2000, Recall: 0.0648, F1: 0.0979\n",
      "Epoch [62/3000], Train Loss: 0.5313\n",
      "Epoch [63/3000],jacquard_loss:0.05144694533762058, hammingloss: 0.2305, Precision: 0.2000, Recall: 0.0648, F1: 0.0979\n",
      "Epoch [63/3000], Train Loss: 0.5267\n",
      "Epoch [64/3000],jacquard_loss:0.05384615384615385, hammingloss: 0.1922, Precision: 0.5185, Recall: 0.0567, F1: 0.1022\n",
      "Epoch [64/3000], Train Loss: 0.5222\n",
      "Epoch [65/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [65/3000], Train Loss: 0.5179\n",
      "Epoch [66/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [66/3000], Train Loss: 0.5138\n",
      "Epoch [67/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [67/3000], Train Loss: 0.5100\n",
      "Epoch [68/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [68/3000], Train Loss: 0.5066\n",
      "Epoch [69/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [69/3000], Train Loss: 0.5036\n",
      "Epoch [70/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [70/3000], Train Loss: 0.5007\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [71/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [71/3000], Train Loss: 0.4982\n",
      "Epoch [72/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [72/3000], Train Loss: 0.4958\n",
      "Epoch [73/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [73/3000], Train Loss: 0.4937\n",
      "Epoch [74/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [74/3000], Train Loss: 0.4917\n",
      "Epoch [75/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [75/3000], Train Loss: 0.4900\n",
      "Epoch [76/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [76/3000], Train Loss: 0.4884\n",
      "Epoch [77/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [77/3000], Train Loss: 0.4869\n",
      "Epoch [78/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [78/3000], Train Loss: 0.4856\n",
      "Epoch [79/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [79/3000], Train Loss: 0.4844\n",
      "Epoch [80/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [80/3000], Train Loss: 0.4833\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [81/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [81/3000], Train Loss: 0.4824\n",
      "Epoch [82/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [82/3000], Train Loss: 0.4814\n",
      "Epoch [83/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [83/3000], Train Loss: 0.4806\n",
      "Epoch [84/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [84/3000], Train Loss: 0.4798\n",
      "Epoch [85/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [85/3000], Train Loss: 0.4791\n",
      "Epoch [86/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [86/3000], Train Loss: 0.4784\n",
      "Epoch [87/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [87/3000], Train Loss: 0.4777\n",
      "Epoch [88/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [88/3000], Train Loss: 0.4771\n",
      "Epoch [89/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [89/3000], Train Loss: 0.4764\n",
      "Epoch [90/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [90/3000], Train Loss: 0.4758\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [91/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [91/3000], Train Loss: 0.4752\n",
      "Epoch [92/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [92/3000], Train Loss: 0.4747\n",
      "Epoch [93/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [93/3000], Train Loss: 0.4741\n",
      "Epoch [94/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [94/3000], Train Loss: 0.4735\n",
      "Epoch [95/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [95/3000], Train Loss: 0.4730\n",
      "Epoch [96/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [96/3000], Train Loss: 0.4725\n",
      "Epoch [97/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [97/3000], Train Loss: 0.4719\n",
      "Epoch [98/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [98/3000], Train Loss: 0.4714\n",
      "Epoch [99/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [99/3000], Train Loss: 0.4709\n",
      "Epoch [100/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [100/3000], Train Loss: 0.4704\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [101/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [101/3000], Train Loss: 0.4699\n",
      "Epoch [102/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [102/3000], Train Loss: 0.4694\n",
      "Epoch [103/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [103/3000], Train Loss: 0.4689\n",
      "Epoch [104/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [104/3000], Train Loss: 0.4684\n",
      "Epoch [105/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [105/3000], Train Loss: 0.4679\n",
      "Epoch [106/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [106/3000], Train Loss: 0.4674\n",
      "Epoch [107/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [107/3000], Train Loss: 0.4670\n",
      "Epoch [108/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [108/3000], Train Loss: 0.4665\n",
      "Epoch [109/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [109/3000], Train Loss: 0.4660\n",
      "Epoch [110/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [110/3000], Train Loss: 0.4656\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [111/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [111/3000], Train Loss: 0.4651\n",
      "Epoch [112/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [112/3000], Train Loss: 0.4647\n",
      "Epoch [113/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [113/3000], Train Loss: 0.4642\n",
      "Epoch [114/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [114/3000], Train Loss: 0.4638\n",
      "Epoch [115/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [115/3000], Train Loss: 0.4633\n",
      "Epoch [116/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [116/3000], Train Loss: 0.4629\n",
      "Epoch [117/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [117/3000], Train Loss: 0.4624\n",
      "Epoch [118/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [118/3000], Train Loss: 0.4620\n",
      "Epoch [119/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [119/3000], Train Loss: 0.4616\n",
      "Epoch [120/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [120/3000], Train Loss: 0.4612\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [121/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [121/3000], Train Loss: 0.4607\n",
      "Epoch [122/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [122/3000], Train Loss: 0.4603\n",
      "Epoch [123/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [123/3000], Train Loss: 0.4599\n",
      "Epoch [124/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [124/3000], Train Loss: 0.4595\n",
      "Epoch [125/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [125/3000], Train Loss: 0.4591\n",
      "Epoch [126/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [126/3000], Train Loss: 0.4587\n",
      "Epoch [127/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [127/3000], Train Loss: 0.4583\n",
      "Epoch [128/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [128/3000], Train Loss: 0.4579\n",
      "Epoch [129/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [129/3000], Train Loss: 0.4575\n",
      "Epoch [130/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [130/3000], Train Loss: 0.4571\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [131/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [131/3000], Train Loss: 0.4568\n",
      "Epoch [132/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [132/3000], Train Loss: 0.4564\n",
      "Epoch [133/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [133/3000], Train Loss: 0.4560\n",
      "Epoch [134/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [134/3000], Train Loss: 0.4556\n",
      "Epoch [135/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [135/3000], Train Loss: 0.4552\n",
      "Epoch [136/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [136/3000], Train Loss: 0.4549\n",
      "Epoch [137/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [137/3000], Train Loss: 0.4545\n",
      "Epoch [138/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [138/3000], Train Loss: 0.4541\n",
      "Epoch [139/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [139/3000], Train Loss: 0.4537\n",
      "Epoch [140/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [140/3000], Train Loss: 0.4534\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [141/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [141/3000], Train Loss: 0.4530\n",
      "Epoch [142/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [142/3000], Train Loss: 0.4527\n",
      "Epoch [143/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [143/3000], Train Loss: 0.4523\n",
      "Epoch [144/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [144/3000], Train Loss: 0.4520\n",
      "Epoch [145/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [145/3000], Train Loss: 0.4517\n",
      "Epoch [146/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [146/3000], Train Loss: 0.4513\n",
      "Epoch [147/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [147/3000], Train Loss: 0.4510\n",
      "Epoch [148/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [148/3000], Train Loss: 0.4507\n",
      "Epoch [149/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [149/3000], Train Loss: 0.4505\n",
      "Epoch [150/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [150/3000], Train Loss: 0.4502\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [151/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [151/3000], Train Loss: 0.4499\n",
      "Epoch [152/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [152/3000], Train Loss: 0.4497\n",
      "Epoch [153/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [153/3000], Train Loss: 0.4494\n",
      "Epoch [154/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [154/3000], Train Loss: 0.4492\n",
      "Epoch [155/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [155/3000], Train Loss: 0.4490\n",
      "Epoch [156/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [156/3000], Train Loss: 0.4488\n",
      "Epoch [157/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [157/3000], Train Loss: 0.4486\n",
      "Epoch [158/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [158/3000], Train Loss: 0.4485\n",
      "Epoch [159/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [159/3000], Train Loss: 0.4483\n",
      "Epoch [160/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [160/3000], Train Loss: 0.4481\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [161/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [161/3000], Train Loss: 0.4480\n",
      "Epoch [162/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [162/3000], Train Loss: 0.4479\n",
      "Epoch [163/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [163/3000], Train Loss: 0.4477\n",
      "Epoch [164/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [164/3000], Train Loss: 0.4476\n",
      "Epoch [165/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [165/3000], Train Loss: 0.4475\n",
      "Epoch [166/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [166/3000], Train Loss: 0.4473\n",
      "Epoch [167/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [167/3000], Train Loss: 0.4472\n",
      "Epoch [168/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [168/3000], Train Loss: 0.4471\n",
      "Epoch [169/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [169/3000], Train Loss: 0.4470\n",
      "Epoch [170/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [170/3000], Train Loss: 0.4468\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [171/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [171/3000], Train Loss: 0.4467\n",
      "Epoch [172/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [172/3000], Train Loss: 0.4465\n",
      "Epoch [173/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [173/3000], Train Loss: 0.4464\n",
      "Epoch [174/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [174/3000], Train Loss: 0.4462\n",
      "Epoch [175/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [175/3000], Train Loss: 0.4461\n",
      "Epoch [176/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [176/3000], Train Loss: 0.4459\n",
      "Epoch [177/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [177/3000], Train Loss: 0.4457\n",
      "Epoch [178/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [178/3000], Train Loss: 0.4455\n",
      "Epoch [179/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [179/3000], Train Loss: 0.4453\n",
      "Epoch [180/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [180/3000], Train Loss: 0.4451\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [181/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [181/3000], Train Loss: 0.4449\n",
      "Epoch [182/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [182/3000], Train Loss: 0.4446\n",
      "Epoch [183/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [183/3000], Train Loss: 0.4444\n",
      "Epoch [184/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [184/3000], Train Loss: 0.4441\n",
      "Epoch [185/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [185/3000], Train Loss: 0.4438\n",
      "Epoch [186/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [186/3000], Train Loss: 0.4434\n",
      "Epoch [187/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [187/3000], Train Loss: 0.4431\n",
      "Epoch [188/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [188/3000], Train Loss: 0.4427\n",
      "Epoch [189/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [189/3000], Train Loss: 0.4423\n",
      "Epoch [190/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [190/3000], Train Loss: 0.4419\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [191/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [191/3000], Train Loss: 0.4414\n",
      "Epoch [192/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [192/3000], Train Loss: 0.4409\n",
      "Epoch [193/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [193/3000], Train Loss: 0.4403\n",
      "Epoch [194/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [194/3000], Train Loss: 0.4397\n",
      "Epoch [195/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [195/3000], Train Loss: 0.4390\n",
      "Epoch [196/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [196/3000], Train Loss: 0.4382\n",
      "Epoch [197/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [197/3000], Train Loss: 0.4373\n",
      "Epoch [198/3000],jacquard_loss:0.0, hammingloss: 0.1930, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch [198/3000], Train Loss: 0.4363\n",
      "Epoch [199/3000],jacquard_loss:0.004048582995951417, hammingloss: 0.1922, Precision: 1.0000, Recall: 0.0040, F1: 0.0081\n",
      "Epoch [199/3000], Train Loss: 0.4352\n",
      "Epoch [200/3000],jacquard_loss:0.004048582995951417, hammingloss: 0.1922, Precision: 1.0000, Recall: 0.0040, F1: 0.0081\n",
      "Epoch [200/3000], Train Loss: 0.4341\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [201/3000],jacquard_loss:0.008064516129032258, hammingloss: 0.1922, Precision: 0.6667, Recall: 0.0081, F1: 0.0160\n",
      "Epoch [201/3000], Train Loss: 0.4327\n",
      "Epoch [202/3000],jacquard_loss:0.016129032258064516, hammingloss: 0.1906, Precision: 0.8000, Recall: 0.0162, F1: 0.0317\n",
      "Epoch [202/3000], Train Loss: 0.4313\n",
      "Epoch [203/3000],jacquard_loss:0.024096385542168676, hammingloss: 0.1898, Precision: 0.7500, Recall: 0.0243, F1: 0.0471\n",
      "Epoch [203/3000], Train Loss: 0.4298\n",
      "Epoch [204/3000],jacquard_loss:0.03187250996015936, hammingloss: 0.1898, Precision: 0.6667, Recall: 0.0324, F1: 0.0618\n",
      "Epoch [204/3000], Train Loss: 0.4282\n",
      "Epoch [205/3000],jacquard_loss:0.03557312252964427, hammingloss: 0.1906, Precision: 0.6000, Recall: 0.0364, F1: 0.0687\n",
      "Epoch [205/3000], Train Loss: 0.4265\n",
      "Epoch [206/3000],jacquard_loss:0.039525691699604744, hammingloss: 0.1898, Precision: 0.6250, Recall: 0.0405, F1: 0.0760\n",
      "Epoch [206/3000], Train Loss: 0.4249\n",
      "Epoch [207/3000],jacquard_loss:0.054901960784313725, hammingloss: 0.1883, Precision: 0.6364, Recall: 0.0567, F1: 0.1041\n",
      "Epoch [207/3000], Train Loss: 0.4233\n",
      "Epoch [208/3000],jacquard_loss:0.054474708171206226, hammingloss: 0.1898, Precision: 0.5833, Recall: 0.0567, F1: 0.1033\n",
      "Epoch [208/3000], Train Loss: 0.4215\n",
      "Epoch [209/3000],jacquard_loss:0.06153846153846154, hammingloss: 0.1906, Precision: 0.5517, Recall: 0.0648, F1: 0.1159\n",
      "Epoch [209/3000], Train Loss: 0.4198\n",
      "Epoch [210/3000],jacquard_loss:0.07662835249042145, hammingloss: 0.1883, Precision: 0.5882, Recall: 0.0810, F1: 0.1423\n",
      "Epoch [210/3000], Train Loss: 0.4179\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [211/3000],jacquard_loss:0.08712121212121213, hammingloss: 0.1883, Precision: 0.5750, Recall: 0.0931, F1: 0.1603\n",
      "Epoch [211/3000], Train Loss: 0.4158\n",
      "Epoch [212/3000],jacquard_loss:0.08745247148288973, hammingloss: 0.1875, Precision: 0.5897, Recall: 0.0931, F1: 0.1608\n",
      "Epoch [212/3000], Train Loss: 0.4137\n",
      "Epoch [213/3000],jacquard_loss:0.0916030534351145, hammingloss: 0.1859, Precision: 0.6154, Recall: 0.0972, F1: 0.1678\n",
      "Epoch [213/3000], Train Loss: 0.4115\n",
      "Epoch [214/3000],jacquard_loss:0.09505703422053231, hammingloss: 0.1859, Precision: 0.6098, Recall: 0.1012, F1: 0.1736\n",
      "Epoch [214/3000], Train Loss: 0.4092\n",
      "Epoch [215/3000],jacquard_loss:0.11406844106463879, hammingloss: 0.1820, Precision: 0.6522, Recall: 0.1215, F1: 0.2048\n",
      "Epoch [215/3000], Train Loss: 0.4069\n",
      "Epoch [216/3000],jacquard_loss:0.12547528517110265, hammingloss: 0.1797, Precision: 0.6735, Recall: 0.1336, F1: 0.2230\n",
      "Epoch [216/3000], Train Loss: 0.4044\n",
      "Epoch [217/3000],jacquard_loss:0.13740458015267176, hammingloss: 0.1766, Precision: 0.7059, Recall: 0.1457, F1: 0.2416\n",
      "Epoch [217/3000], Train Loss: 0.4020\n",
      "Epoch [218/3000],jacquard_loss:0.14942528735632185, hammingloss: 0.1734, Precision: 0.7358, Recall: 0.1579, F1: 0.2600\n",
      "Epoch [218/3000], Train Loss: 0.3996\n",
      "Epoch [219/3000],jacquard_loss:0.1444866920152091, hammingloss: 0.1758, Precision: 0.7037, Recall: 0.1538, F1: 0.2525\n",
      "Epoch [219/3000], Train Loss: 0.3972\n",
      "Epoch [220/3000],jacquard_loss:0.15267175572519084, hammingloss: 0.1734, Precision: 0.7273, Recall: 0.1619, F1: 0.2649\n",
      "Epoch [220/3000], Train Loss: 0.3948\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [221/3000],jacquard_loss:0.15648854961832062, hammingloss: 0.1727, Precision: 0.7321, Recall: 0.1660, F1: 0.2706\n",
      "Epoch [221/3000], Train Loss: 0.3926\n",
      "Epoch [222/3000],jacquard_loss:0.1596958174904943, hammingloss: 0.1727, Precision: 0.7241, Recall: 0.1700, F1: 0.2754\n",
      "Epoch [222/3000], Train Loss: 0.3904\n",
      "Epoch [223/3000],jacquard_loss:0.1590909090909091, hammingloss: 0.1734, Precision: 0.7119, Recall: 0.1700, F1: 0.2745\n",
      "Epoch [223/3000], Train Loss: 0.3884\n",
      "Epoch [224/3000],jacquard_loss:0.1590909090909091, hammingloss: 0.1734, Precision: 0.7119, Recall: 0.1700, F1: 0.2745\n",
      "Epoch [224/3000], Train Loss: 0.3866\n",
      "Epoch [225/3000],jacquard_loss:0.16287878787878787, hammingloss: 0.1727, Precision: 0.7167, Recall: 0.1741, F1: 0.2801\n",
      "Epoch [225/3000], Train Loss: 0.3848\n",
      "Epoch [226/3000],jacquard_loss:0.16226415094339622, hammingloss: 0.1734, Precision: 0.7049, Recall: 0.1741, F1: 0.2792\n",
      "Epoch [226/3000], Train Loss: 0.3830\n",
      "Epoch [227/3000],jacquard_loss:0.16226415094339622, hammingloss: 0.1734, Precision: 0.7049, Recall: 0.1741, F1: 0.2792\n",
      "Epoch [227/3000], Train Loss: 0.3814\n",
      "Epoch [228/3000],jacquard_loss:0.16917293233082706, hammingloss: 0.1727, Precision: 0.7031, Recall: 0.1822, F1: 0.2894\n",
      "Epoch [228/3000], Train Loss: 0.3797\n",
      "Epoch [229/3000],jacquard_loss:0.16917293233082706, hammingloss: 0.1727, Precision: 0.7031, Recall: 0.1822, F1: 0.2894\n",
      "Epoch [229/3000], Train Loss: 0.3782\n",
      "Epoch [230/3000],jacquard_loss:0.16853932584269662, hammingloss: 0.1734, Precision: 0.6923, Recall: 0.1822, F1: 0.2885\n",
      "Epoch [230/3000], Train Loss: 0.3768\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [231/3000],jacquard_loss:0.17228464419475656, hammingloss: 0.1727, Precision: 0.6970, Recall: 0.1862, F1: 0.2939\n",
      "Epoch [231/3000], Train Loss: 0.3753\n",
      "Epoch [232/3000],jacquard_loss:0.17164179104477612, hammingloss: 0.1734, Precision: 0.6866, Recall: 0.1862, F1: 0.2930\n",
      "Epoch [232/3000], Train Loss: 0.3741\n",
      "Epoch [233/3000],jacquard_loss:0.17537313432835822, hammingloss: 0.1727, Precision: 0.6912, Recall: 0.1903, F1: 0.2984\n",
      "Epoch [233/3000], Train Loss: 0.3727\n",
      "Epoch [234/3000],jacquard_loss:0.17100371747211895, hammingloss: 0.1742, Precision: 0.6765, Recall: 0.1862, F1: 0.2921\n",
      "Epoch [234/3000], Train Loss: 0.3714\n",
      "Epoch [235/3000],jacquard_loss:0.17472118959107807, hammingloss: 0.1734, Precision: 0.6812, Recall: 0.1903, F1: 0.2975\n",
      "Epoch [235/3000], Train Loss: 0.3701\n",
      "Epoch [236/3000],jacquard_loss:0.17472118959107807, hammingloss: 0.1734, Precision: 0.6812, Recall: 0.1903, F1: 0.2975\n",
      "Epoch [236/3000], Train Loss: 0.3687\n",
      "Epoch [237/3000],jacquard_loss:0.17407407407407408, hammingloss: 0.1742, Precision: 0.6714, Recall: 0.1903, F1: 0.2965\n",
      "Epoch [237/3000], Train Loss: 0.3675\n",
      "Epoch [238/3000],jacquard_loss:0.16974169741697417, hammingloss: 0.1758, Precision: 0.6571, Recall: 0.1862, F1: 0.2902\n",
      "Epoch [238/3000], Train Loss: 0.3663\n",
      "Epoch [239/3000],jacquard_loss:0.17216117216117216, hammingloss: 0.1766, Precision: 0.6438, Recall: 0.1903, F1: 0.2937\n",
      "Epoch [239/3000], Train Loss: 0.3650\n",
      "Epoch [240/3000],jacquard_loss:0.17343173431734318, hammingloss: 0.1750, Precision: 0.6620, Recall: 0.1903, F1: 0.2956\n",
      "Epoch [240/3000], Train Loss: 0.3637\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [241/3000],jacquard_loss:0.17582417582417584, hammingloss: 0.1758, Precision: 0.6486, Recall: 0.1943, F1: 0.2991\n",
      "Epoch [241/3000], Train Loss: 0.3624\n",
      "Epoch [242/3000],jacquard_loss:0.18315018315018314, hammingloss: 0.1742, Precision: 0.6579, Recall: 0.2024, F1: 0.3096\n",
      "Epoch [242/3000], Train Loss: 0.3613\n",
      "Epoch [243/3000],jacquard_loss:0.1948529411764706, hammingloss: 0.1711, Precision: 0.6795, Recall: 0.2146, F1: 0.3262\n",
      "Epoch [243/3000], Train Loss: 0.3600\n",
      "Epoch [244/3000],jacquard_loss:0.19852941176470587, hammingloss: 0.1703, Precision: 0.6835, Recall: 0.2186, F1: 0.3313\n",
      "Epoch [244/3000], Train Loss: 0.3588\n",
      "Epoch [245/3000],jacquard_loss:0.20146520146520147, hammingloss: 0.1703, Precision: 0.6790, Recall: 0.2227, F1: 0.3354\n",
      "Epoch [245/3000], Train Loss: 0.3574\n",
      "Epoch [246/3000],jacquard_loss:0.1978021978021978, hammingloss: 0.1711, Precision: 0.6750, Recall: 0.2186, F1: 0.3303\n",
      "Epoch [246/3000], Train Loss: 0.3561\n",
      "Epoch [247/3000],jacquard_loss:0.20512820512820512, hammingloss: 0.1695, Precision: 0.6829, Recall: 0.2267, F1: 0.3404\n",
      "Epoch [247/3000], Train Loss: 0.3548\n",
      "Epoch [248/3000],jacquard_loss:0.20072992700729927, hammingloss: 0.1711, Precision: 0.6707, Recall: 0.2227, F1: 0.3343\n",
      "Epoch [248/3000], Train Loss: 0.3536\n",
      "Epoch [249/3000],jacquard_loss:0.21532846715328466, hammingloss: 0.1680, Precision: 0.6860, Recall: 0.2389, F1: 0.3544\n",
      "Epoch [249/3000], Train Loss: 0.3521\n",
      "Epoch [250/3000],jacquard_loss:0.20588235294117646, hammingloss: 0.1688, Precision: 0.6914, Recall: 0.2267, F1: 0.3415\n",
      "Epoch [250/3000], Train Loss: 0.3511\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [251/3000],jacquard_loss:0.213768115942029, hammingloss: 0.1695, Precision: 0.6705, Recall: 0.2389, F1: 0.3522\n",
      "Epoch [251/3000], Train Loss: 0.3489\n",
      "Epoch [252/3000],jacquard_loss:0.2109090909090909, hammingloss: 0.1695, Precision: 0.6744, Recall: 0.2348, F1: 0.3483\n",
      "Epoch [252/3000], Train Loss: 0.3481\n",
      "Epoch [253/3000],jacquard_loss:0.22939068100358423, hammingloss: 0.1680, Precision: 0.6667, Recall: 0.2591, F1: 0.3732\n",
      "Epoch [253/3000], Train Loss: 0.3459\n",
      "Epoch [254/3000],jacquard_loss:0.22142857142857142, hammingloss: 0.1703, Precision: 0.6526, Recall: 0.2510, F1: 0.3626\n",
      "Epoch [254/3000], Train Loss: 0.3449\n",
      "Epoch [255/3000],jacquard_loss:0.24295774647887325, hammingloss: 0.1680, Precision: 0.6509, Recall: 0.2794, F1: 0.3909\n",
      "Epoch [255/3000], Train Loss: 0.3426\n",
      "Epoch [256/3000],jacquard_loss:0.23404255319148937, hammingloss: 0.1688, Precision: 0.6535, Recall: 0.2672, F1: 0.3793\n",
      "Epoch [256/3000], Train Loss: 0.3416\n",
      "Epoch [257/3000],jacquard_loss:0.24041811846689895, hammingloss: 0.1703, Precision: 0.6330, Recall: 0.2794, F1: 0.3876\n",
      "Epoch [257/3000], Train Loss: 0.3392\n",
      "Epoch [258/3000],jacquard_loss:0.25, hammingloss: 0.1664, Precision: 0.6574, Recall: 0.2874, F1: 0.4000\n",
      "Epoch [258/3000], Train Loss: 0.3381\n",
      "Epoch [259/3000],jacquard_loss:0.25435540069686413, hammingloss: 0.1672, Precision: 0.6460, Recall: 0.2955, F1: 0.4056\n",
      "Epoch [259/3000], Train Loss: 0.3355\n",
      "Epoch [260/3000],jacquard_loss:0.25874125874125875, hammingloss: 0.1656, Precision: 0.6549, Recall: 0.2996, F1: 0.4111\n",
      "Epoch [260/3000], Train Loss: 0.3343\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [261/3000],jacquard_loss:0.27972027972027974, hammingloss: 0.1609, Precision: 0.6723, Recall: 0.3239, F1: 0.4372\n",
      "Epoch [261/3000], Train Loss: 0.3317\n",
      "Epoch [262/3000],jacquard_loss:0.2857142857142857, hammingloss: 0.1602, Precision: 0.6721, Recall: 0.3320, F1: 0.4444\n",
      "Epoch [262/3000], Train Loss: 0.3302\n",
      "Epoch [263/3000],jacquard_loss:0.2787456445993031, hammingloss: 0.1617, Precision: 0.6667, Recall: 0.3239, F1: 0.4360\n",
      "Epoch [263/3000], Train Loss: 0.3279\n",
      "Epoch [264/3000],jacquard_loss:0.2857142857142857, hammingloss: 0.1602, Precision: 0.6721, Recall: 0.3320, F1: 0.4444\n",
      "Epoch [264/3000], Train Loss: 0.3261\n",
      "Epoch [265/3000],jacquard_loss:0.28771929824561404, hammingloss: 0.1586, Precision: 0.6833, Recall: 0.3320, F1: 0.4469\n",
      "Epoch [265/3000], Train Loss: 0.3241\n",
      "Epoch [266/3000],jacquard_loss:0.28421052631578947, hammingloss: 0.1594, Precision: 0.6807, Recall: 0.3279, F1: 0.4426\n",
      "Epoch [266/3000], Train Loss: 0.3224\n",
      "Epoch [267/3000],jacquard_loss:0.29473684210526313, hammingloss: 0.1570, Precision: 0.6885, Recall: 0.3401, F1: 0.4553\n",
      "Epoch [267/3000], Train Loss: 0.3202\n",
      "Epoch [268/3000],jacquard_loss:0.30141843971631205, hammingloss: 0.1539, Precision: 0.7083, Recall: 0.3441, F1: 0.4632\n",
      "Epoch [268/3000], Train Loss: 0.3185\n",
      "Epoch [269/3000],jacquard_loss:0.2978723404255319, hammingloss: 0.1547, Precision: 0.7059, Recall: 0.3401, F1: 0.4590\n",
      "Epoch [269/3000], Train Loss: 0.3158\n",
      "Epoch [270/3000],jacquard_loss:0.3010752688172043, hammingloss: 0.1523, Precision: 0.7241, Recall: 0.3401, F1: 0.4628\n",
      "Epoch [270/3000], Train Loss: 0.3143\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [271/3000],jacquard_loss:0.30604982206405695, hammingloss: 0.1523, Precision: 0.7167, Recall: 0.3482, F1: 0.4687\n",
      "Epoch [271/3000], Train Loss: 0.3112\n",
      "Epoch [272/3000],jacquard_loss:0.3118279569892473, hammingloss: 0.1500, Precision: 0.7311, Recall: 0.3522, F1: 0.4754\n",
      "Epoch [272/3000], Train Loss: 0.3101\n",
      "Epoch [273/3000],jacquard_loss:0.3154121863799283, hammingloss: 0.1492, Precision: 0.7333, Recall: 0.3563, F1: 0.4796\n",
      "Epoch [273/3000], Train Loss: 0.3069\n",
      "Epoch [274/3000],jacquard_loss:0.3129496402877698, hammingloss: 0.1492, Precision: 0.7373, Recall: 0.3522, F1: 0.4767\n",
      "Epoch [274/3000], Train Loss: 0.3057\n",
      "Epoch [275/3000],jacquard_loss:0.3154121863799283, hammingloss: 0.1492, Precision: 0.7333, Recall: 0.3563, F1: 0.4796\n",
      "Epoch [275/3000], Train Loss: 0.3024\n",
      "Epoch [276/3000],jacquard_loss:0.32014388489208634, hammingloss: 0.1477, Precision: 0.7417, Recall: 0.3603, F1: 0.4850\n",
      "Epoch [276/3000], Train Loss: 0.3010\n",
      "Epoch [277/3000],jacquard_loss:0.3237410071942446, hammingloss: 0.1469, Precision: 0.7438, Recall: 0.3644, F1: 0.4891\n",
      "Epoch [277/3000], Train Loss: 0.2983\n",
      "Epoch [278/3000],jacquard_loss:0.3212996389891697, hammingloss: 0.1469, Precision: 0.7479, Recall: 0.3603, F1: 0.4863\n",
      "Epoch [278/3000], Train Loss: 0.2968\n",
      "Epoch [279/3000],jacquard_loss:0.33212996389891697, hammingloss: 0.1445, Precision: 0.7541, Recall: 0.3725, F1: 0.4986\n",
      "Epoch [279/3000], Train Loss: 0.2943\n",
      "Epoch [280/3000],jacquard_loss:0.32971014492753625, hammingloss: 0.1445, Precision: 0.7583, Recall: 0.3684, F1: 0.4959\n",
      "Epoch [280/3000], Train Loss: 0.2926\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [281/3000],jacquard_loss:0.33090909090909093, hammingloss: 0.1437, Precision: 0.7647, Recall: 0.3684, F1: 0.4973\n",
      "Epoch [281/3000], Train Loss: 0.2907\n",
      "Epoch [282/3000],jacquard_loss:0.32727272727272727, hammingloss: 0.1445, Precision: 0.7627, Recall: 0.3644, F1: 0.4932\n",
      "Epoch [282/3000], Train Loss: 0.2889\n",
      "Epoch [283/3000],jacquard_loss:0.3333333333333333, hammingloss: 0.1437, Precision: 0.7603, Recall: 0.3725, F1: 0.5000\n",
      "Epoch [283/3000], Train Loss: 0.2870\n",
      "Epoch [284/3000],jacquard_loss:0.3333333333333333, hammingloss: 0.1437, Precision: 0.7603, Recall: 0.3725, F1: 0.5000\n",
      "Epoch [284/3000], Train Loss: 0.2851\n",
      "Epoch [285/3000],jacquard_loss:0.33454545454545453, hammingloss: 0.1430, Precision: 0.7667, Recall: 0.3725, F1: 0.5014\n",
      "Epoch [285/3000], Train Loss: 0.2835\n",
      "Epoch [286/3000],jacquard_loss:0.3357664233576642, hammingloss: 0.1422, Precision: 0.7731, Recall: 0.3725, F1: 0.5027\n",
      "Epoch [286/3000], Train Loss: 0.2818\n",
      "Epoch [287/3000],jacquard_loss:0.34545454545454546, hammingloss: 0.1406, Precision: 0.7724, Recall: 0.3846, F1: 0.5135\n",
      "Epoch [287/3000], Train Loss: 0.2800\n",
      "Epoch [288/3000],jacquard_loss:0.3442028985507246, hammingloss: 0.1414, Precision: 0.7661, Recall: 0.3846, F1: 0.5121\n",
      "Epoch [288/3000], Train Loss: 0.2784\n",
      "Epoch [289/3000],jacquard_loss:0.35036496350364965, hammingloss: 0.1391, Precision: 0.7805, Recall: 0.3887, F1: 0.5189\n",
      "Epoch [289/3000], Train Loss: 0.2765\n",
      "Epoch [290/3000],jacquard_loss:0.3613138686131387, hammingloss: 0.1367, Precision: 0.7857, Recall: 0.4008, F1: 0.5308\n",
      "Epoch [290/3000], Train Loss: 0.2747\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [291/3000],jacquard_loss:0.3626373626373626, hammingloss: 0.1359, Precision: 0.7920, Recall: 0.4008, F1: 0.5323\n",
      "Epoch [291/3000], Train Loss: 0.2732\n",
      "Epoch [292/3000],jacquard_loss:0.3574007220216607, hammingloss: 0.1391, Precision: 0.7674, Recall: 0.4008, F1: 0.5266\n",
      "Epoch [292/3000], Train Loss: 0.2720\n",
      "Epoch [293/3000],jacquard_loss:0.36363636363636365, hammingloss: 0.1367, Precision: 0.7812, Recall: 0.4049, F1: 0.5333\n",
      "Epoch [293/3000], Train Loss: 0.2699\n",
      "Epoch [294/3000],jacquard_loss:0.36363636363636365, hammingloss: 0.1367, Precision: 0.7812, Recall: 0.4049, F1: 0.5333\n",
      "Epoch [294/3000], Train Loss: 0.2683\n",
      "Epoch [295/3000],jacquard_loss:0.36200716845878134, hammingloss: 0.1391, Precision: 0.7594, Recall: 0.4089, F1: 0.5316\n",
      "Epoch [295/3000], Train Loss: 0.2664\n",
      "Epoch [296/3000],jacquard_loss:0.36823104693140796, hammingloss: 0.1367, Precision: 0.7727, Recall: 0.4130, F1: 0.5383\n",
      "Epoch [296/3000], Train Loss: 0.2646\n",
      "Epoch [297/3000],jacquard_loss:0.37050359712230213, hammingloss: 0.1367, Precision: 0.7687, Recall: 0.4170, F1: 0.5407\n",
      "Epoch [297/3000], Train Loss: 0.2630\n",
      "Epoch [298/3000],jacquard_loss:0.38489208633093525, hammingloss: 0.1336, Precision: 0.7754, Recall: 0.4332, F1: 0.5558\n",
      "Epoch [298/3000], Train Loss: 0.2611\n",
      "Epoch [299/3000],jacquard_loss:0.3829787234042553, hammingloss: 0.1359, Precision: 0.7552, Recall: 0.4372, F1: 0.5538\n",
      "Epoch [299/3000], Train Loss: 0.2593\n",
      "Epoch [300/3000],jacquard_loss:0.392226148409894, hammingloss: 0.1344, Precision: 0.7551, Recall: 0.4494, F1: 0.5635\n",
      "Epoch [300/3000], Train Loss: 0.2579\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [301/3000],jacquard_loss:0.3914590747330961, hammingloss: 0.1336, Precision: 0.7639, Recall: 0.4453, F1: 0.5627\n",
      "Epoch [301/3000], Train Loss: 0.2567\n",
      "Epoch [302/3000],jacquard_loss:0.3971631205673759, hammingloss: 0.1328, Precision: 0.7619, Recall: 0.4534, F1: 0.5685\n",
      "Epoch [302/3000], Train Loss: 0.2547\n",
      "Epoch [303/3000],jacquard_loss:0.39372822299651566, hammingloss: 0.1359, Precision: 0.7386, Recall: 0.4575, F1: 0.5650\n",
      "Epoch [303/3000], Train Loss: 0.2532\n",
      "Epoch [304/3000],jacquard_loss:0.40559440559440557, hammingloss: 0.1328, Precision: 0.7484, Recall: 0.4696, F1: 0.5771\n",
      "Epoch [304/3000], Train Loss: 0.2516\n",
      "Epoch [305/3000],jacquard_loss:0.4105263157894737, hammingloss: 0.1313, Precision: 0.7548, Recall: 0.4737, F1: 0.5821\n",
      "Epoch [305/3000], Train Loss: 0.2495\n",
      "Epoch [306/3000],jacquard_loss:0.4070175438596491, hammingloss: 0.1320, Precision: 0.7532, Recall: 0.4696, F1: 0.5786\n",
      "Epoch [306/3000], Train Loss: 0.2488\n",
      "Epoch [307/3000],jacquard_loss:0.4201388888888889, hammingloss: 0.1305, Precision: 0.7469, Recall: 0.4899, F1: 0.5917\n",
      "Epoch [307/3000], Train Loss: 0.2468\n",
      "Epoch [308/3000],jacquard_loss:0.41754385964912283, hammingloss: 0.1297, Precision: 0.7580, Recall: 0.4818, F1: 0.5891\n",
      "Epoch [308/3000], Train Loss: 0.2447\n",
      "Epoch [309/3000],jacquard_loss:0.42857142857142855, hammingloss: 0.1281, Precision: 0.7546, Recall: 0.4980, F1: 0.6000\n",
      "Epoch [309/3000], Train Loss: 0.2432\n",
      "Epoch [310/3000],jacquard_loss:0.4421768707482993, hammingloss: 0.1281, Precision: 0.7345, Recall: 0.5263, F1: 0.6132\n",
      "Epoch [310/3000], Train Loss: 0.2423\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [311/3000],jacquard_loss:0.44982698961937717, hammingloss: 0.1242, Precision: 0.7558, Recall: 0.5263, F1: 0.6205\n",
      "Epoch [311/3000], Train Loss: 0.2405\n",
      "Epoch [312/3000],jacquard_loss:0.42955326460481097, hammingloss: 0.1297, Precision: 0.7396, Recall: 0.5061, F1: 0.6010\n",
      "Epoch [312/3000], Train Loss: 0.2385\n",
      "Epoch [313/3000],jacquard_loss:0.46048109965635736, hammingloss: 0.1227, Precision: 0.7528, Recall: 0.5425, F1: 0.6306\n",
      "Epoch [313/3000], Train Loss: 0.2373\n",
      "Epoch [314/3000],jacquard_loss:0.4623287671232877, hammingloss: 0.1227, Precision: 0.7500, Recall: 0.5466, F1: 0.6323\n",
      "Epoch [314/3000], Train Loss: 0.2352\n",
      "Epoch [315/3000],jacquard_loss:0.4794520547945205, hammingloss: 0.1187, Precision: 0.7568, Recall: 0.5668, F1: 0.6481\n",
      "Epoch [315/3000], Train Loss: 0.2345\n",
      "Epoch [316/3000],jacquard_loss:0.4915254237288136, hammingloss: 0.1172, Precision: 0.7513, Recall: 0.5870, F1: 0.6591\n",
      "Epoch [316/3000], Train Loss: 0.2329\n",
      "Epoch [317/3000],jacquard_loss:0.4724137931034483, hammingloss: 0.1195, Precision: 0.7611, Recall: 0.5547, F1: 0.6417\n",
      "Epoch [317/3000], Train Loss: 0.2318\n",
      "Epoch [318/3000],jacquard_loss:0.4948805460750853, hammingloss: 0.1156, Precision: 0.7592, Recall: 0.5870, F1: 0.6621\n",
      "Epoch [318/3000], Train Loss: 0.2301\n",
      "Epoch [319/3000],jacquard_loss:0.4931972789115646, hammingloss: 0.1164, Precision: 0.7552, Recall: 0.5870, F1: 0.6606\n",
      "Epoch [319/3000], Train Loss: 0.2283\n",
      "Epoch [320/3000],jacquard_loss:0.5051903114186851, hammingloss: 0.1117, Precision: 0.7766, Recall: 0.5911, F1: 0.6713\n",
      "Epoch [320/3000], Train Loss: 0.2267\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [321/3000],jacquard_loss:0.5085910652920962, hammingloss: 0.1117, Precision: 0.7708, Recall: 0.5992, F1: 0.6743\n",
      "Epoch [321/3000], Train Loss: 0.2257\n",
      "Epoch [322/3000],jacquard_loss:0.5102739726027398, hammingloss: 0.1117, Precision: 0.7680, Recall: 0.6032, F1: 0.6757\n",
      "Epoch [322/3000], Train Loss: 0.2242\n",
      "Epoch [323/3000],jacquard_loss:0.5277777777777778, hammingloss: 0.1062, Precision: 0.7876, Recall: 0.6154, F1: 0.6909\n",
      "Epoch [323/3000], Train Loss: 0.2232\n",
      "Epoch [324/3000],jacquard_loss:0.5206896551724138, hammingloss: 0.1086, Precision: 0.7784, Recall: 0.6113, F1: 0.6848\n",
      "Epoch [324/3000], Train Loss: 0.2222\n",
      "Epoch [325/3000],jacquard_loss:0.5429553264604811, hammingloss: 0.1039, Precision: 0.7822, Recall: 0.6397, F1: 0.7038\n",
      "Epoch [325/3000], Train Loss: 0.2205\n",
      "Epoch [326/3000],jacquard_loss:0.5259515570934256, hammingloss: 0.1070, Precision: 0.7835, Recall: 0.6154, F1: 0.6893\n",
      "Epoch [326/3000], Train Loss: 0.2189\n",
      "Epoch [327/3000],jacquard_loss:0.5395189003436426, hammingloss: 0.1047, Precision: 0.7811, Recall: 0.6356, F1: 0.7009\n",
      "Epoch [327/3000], Train Loss: 0.2175\n",
      "Epoch [328/3000],jacquard_loss:0.552901023890785, hammingloss: 0.1023, Precision: 0.7788, Recall: 0.6559, F1: 0.7121\n",
      "Epoch [328/3000], Train Loss: 0.2168\n",
      "Epoch [329/3000],jacquard_loss:0.5448275862068965, hammingloss: 0.1031, Precision: 0.7861, Recall: 0.6397, F1: 0.7054\n",
      "Epoch [329/3000], Train Loss: 0.2142\n",
      "Epoch [330/3000],jacquard_loss:0.5517241379310345, hammingloss: 0.1016, Precision: 0.7882, Recall: 0.6478, F1: 0.7111\n",
      "Epoch [330/3000], Train Loss: 0.2132\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [331/3000],jacquard_loss:0.5567010309278351, hammingloss: 0.1008, Precision: 0.7864, Recall: 0.6559, F1: 0.7152\n",
      "Epoch [331/3000], Train Loss: 0.2112\n",
      "Epoch [332/3000],jacquard_loss:0.5674740484429066, hammingloss: 0.0977, Precision: 0.7961, Recall: 0.6640, F1: 0.7241\n",
      "Epoch [332/3000], Train Loss: 0.2100\n",
      "Epoch [333/3000],jacquard_loss:0.5517241379310345, hammingloss: 0.1016, Precision: 0.7882, Recall: 0.6478, F1: 0.7111\n",
      "Epoch [333/3000], Train Loss: 0.2083\n",
      "Epoch [334/3000],jacquard_loss:0.563573883161512, hammingloss: 0.0992, Precision: 0.7885, Recall: 0.6640, F1: 0.7209\n",
      "Epoch [334/3000], Train Loss: 0.2072\n",
      "Epoch [335/3000],jacquard_loss:0.5694444444444444, hammingloss: 0.0969, Precision: 0.8000, Recall: 0.6640, F1: 0.7257\n",
      "Epoch [335/3000], Train Loss: 0.2060\n",
      "Epoch [336/3000],jacquard_loss:0.5804195804195804, hammingloss: 0.0938, Precision: 0.8098, Recall: 0.6721, F1: 0.7345\n",
      "Epoch [336/3000], Train Loss: 0.2051\n",
      "Epoch [337/3000],jacquard_loss:0.5896551724137931, hammingloss: 0.0930, Precision: 0.7991, Recall: 0.6923, F1: 0.7419\n",
      "Epoch [337/3000], Train Loss: 0.2041\n",
      "Epoch [338/3000],jacquard_loss:0.578397212543554, hammingloss: 0.0945, Precision: 0.8058, Recall: 0.6721, F1: 0.7329\n",
      "Epoch [338/3000], Train Loss: 0.2026\n",
      "Epoch [339/3000],jacquard_loss:0.5773195876288659, hammingloss: 0.0961, Precision: 0.7925, Recall: 0.6802, F1: 0.7320\n",
      "Epoch [339/3000], Train Loss: 0.2014\n",
      "Epoch [340/3000],jacquard_loss:0.5986159169550173, hammingloss: 0.0906, Precision: 0.8047, Recall: 0.7004, F1: 0.7489\n",
      "Epoch [340/3000], Train Loss: 0.1998\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [341/3000],jacquard_loss:0.5964912280701754, hammingloss: 0.0898, Precision: 0.8173, Recall: 0.6883, F1: 0.7473\n",
      "Epoch [341/3000], Train Loss: 0.1978\n",
      "Epoch [342/3000],jacquard_loss:0.5896551724137931, hammingloss: 0.0930, Precision: 0.7991, Recall: 0.6923, F1: 0.7419\n",
      "Epoch [342/3000], Train Loss: 0.1970\n",
      "Epoch [343/3000],jacquard_loss:0.6006944444444444, hammingloss: 0.0898, Precision: 0.8084, Recall: 0.7004, F1: 0.7505\n",
      "Epoch [343/3000], Train Loss: 0.1950\n",
      "Epoch [344/3000],jacquard_loss:0.6097560975609756, hammingloss: 0.0875, Precision: 0.8140, Recall: 0.7085, F1: 0.7576\n",
      "Epoch [344/3000], Train Loss: 0.1941\n",
      "Epoch [345/3000],jacquard_loss:0.6048951048951049, hammingloss: 0.0883, Precision: 0.8160, Recall: 0.7004, F1: 0.7538\n",
      "Epoch [345/3000], Train Loss: 0.1920\n",
      "Epoch [346/3000],jacquard_loss:0.624561403508772, hammingloss: 0.0836, Precision: 0.8241, Recall: 0.7206, F1: 0.7689\n",
      "Epoch [346/3000], Train Loss: 0.1909\n",
      "Epoch [347/3000],jacquard_loss:0.6202090592334495, hammingloss: 0.0852, Precision: 0.8165, Recall: 0.7206, F1: 0.7656\n",
      "Epoch [347/3000], Train Loss: 0.1895\n",
      "Epoch [348/3000],jacquard_loss:0.6175438596491228, hammingloss: 0.0852, Precision: 0.8224, Recall: 0.7126, F1: 0.7636\n",
      "Epoch [348/3000], Train Loss: 0.1886\n",
      "Epoch [349/3000],jacquard_loss:0.6373239436619719, hammingloss: 0.0805, Precision: 0.8303, Recall: 0.7328, F1: 0.7785\n",
      "Epoch [349/3000], Train Loss: 0.1866\n",
      "Epoch [350/3000],jacquard_loss:0.631578947368421, hammingloss: 0.0820, Precision: 0.8257, Recall: 0.7287, F1: 0.7742\n",
      "Epoch [350/3000], Train Loss: 0.1859\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [351/3000],jacquard_loss:0.6360424028268551, hammingloss: 0.0805, Precision: 0.8333, Recall: 0.7287, F1: 0.7775\n",
      "Epoch [351/3000], Train Loss: 0.1843\n",
      "Epoch [352/3000],jacquard_loss:0.6382978723404256, hammingloss: 0.0797, Precision: 0.8372, Recall: 0.7287, F1: 0.7792\n",
      "Epoch [352/3000], Train Loss: 0.1831\n",
      "Epoch [353/3000],jacquard_loss:0.6431095406360424, hammingloss: 0.0789, Precision: 0.8349, Recall: 0.7368, F1: 0.7828\n",
      "Epoch [353/3000], Train Loss: 0.1819\n",
      "Epoch [354/3000],jacquard_loss:0.6350877192982456, hammingloss: 0.0813, Precision: 0.8265, Recall: 0.7328, F1: 0.7768\n",
      "Epoch [354/3000], Train Loss: 0.1805\n",
      "Epoch [355/3000],jacquard_loss:0.6466431095406361, hammingloss: 0.0781, Precision: 0.8356, Recall: 0.7409, F1: 0.7854\n",
      "Epoch [355/3000], Train Loss: 0.1790\n",
      "Epoch [356/3000],jacquard_loss:0.6373239436619719, hammingloss: 0.0805, Precision: 0.8303, Recall: 0.7328, F1: 0.7785\n",
      "Epoch [356/3000], Train Loss: 0.1780\n",
      "Epoch [357/3000],jacquard_loss:0.6537102473498233, hammingloss: 0.0766, Precision: 0.8371, Recall: 0.7490, F1: 0.7906\n",
      "Epoch [357/3000], Train Loss: 0.1762\n",
      "Epoch [358/3000],jacquard_loss:0.6405693950177936, hammingloss: 0.0789, Precision: 0.8411, Recall: 0.7287, F1: 0.7809\n",
      "Epoch [358/3000], Train Loss: 0.1762\n",
      "Epoch [359/3000],jacquard_loss:0.6524822695035462, hammingloss: 0.0766, Precision: 0.8402, Recall: 0.7449, F1: 0.7897\n",
      "Epoch [359/3000], Train Loss: 0.1742\n",
      "Epoch [360/3000],jacquard_loss:0.6503496503496503, hammingloss: 0.0781, Precision: 0.8267, Recall: 0.7530, F1: 0.7881\n",
      "Epoch [360/3000], Train Loss: 0.1743\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [361/3000],jacquard_loss:0.6433566433566433, hammingloss: 0.0797, Precision: 0.8251, Recall: 0.7449, F1: 0.7830\n",
      "Epoch [361/3000], Train Loss: 0.1727\n",
      "Epoch [362/3000],jacquard_loss:0.6466431095406361, hammingloss: 0.0781, Precision: 0.8356, Recall: 0.7409, F1: 0.7854\n",
      "Epoch [362/3000], Train Loss: 0.1711\n",
      "Epoch [363/3000],jacquard_loss:0.6514084507042254, hammingloss: 0.0773, Precision: 0.8333, Recall: 0.7490, F1: 0.7889\n",
      "Epoch [363/3000], Train Loss: 0.1700\n",
      "Epoch [364/3000],jacquard_loss:0.6678445229681979, hammingloss: 0.0734, Precision: 0.8400, Recall: 0.7652, F1: 0.8008\n",
      "Epoch [364/3000], Train Loss: 0.1686\n",
      "Epoch [365/3000],jacquard_loss:0.6514084507042254, hammingloss: 0.0773, Precision: 0.8333, Recall: 0.7490, F1: 0.7889\n",
      "Epoch [365/3000], Train Loss: 0.1671\n",
      "Epoch [366/3000],jacquard_loss:0.6584507042253521, hammingloss: 0.0758, Precision: 0.8348, Recall: 0.7571, F1: 0.7941\n",
      "Epoch [366/3000], Train Loss: 0.1654\n",
      "Epoch [367/3000],jacquard_loss:0.6584507042253521, hammingloss: 0.0758, Precision: 0.8348, Recall: 0.7571, F1: 0.7941\n",
      "Epoch [367/3000], Train Loss: 0.1650\n",
      "Epoch [368/3000],jacquard_loss:0.656140350877193, hammingloss: 0.0766, Precision: 0.8311, Recall: 0.7571, F1: 0.7924\n",
      "Epoch [368/3000], Train Loss: 0.1633\n",
      "Epoch [369/3000],jacquard_loss:0.6515679442508711, hammingloss: 0.0781, Precision: 0.8238, Recall: 0.7571, F1: 0.7890\n",
      "Epoch [369/3000], Train Loss: 0.1623\n",
      "Epoch [370/3000],jacquard_loss:0.6619718309859155, hammingloss: 0.0750, Precision: 0.8356, Recall: 0.7611, F1: 0.7966\n",
      "Epoch [370/3000], Train Loss: 0.1611\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [371/3000],jacquard_loss:0.6714285714285714, hammingloss: 0.0719, Precision: 0.8507, Recall: 0.7611, F1: 0.8034\n",
      "Epoch [371/3000], Train Loss: 0.1600\n",
      "Epoch [372/3000],jacquard_loss:0.6666666666666666, hammingloss: 0.0742, Precision: 0.8333, Recall: 0.7692, F1: 0.8000\n",
      "Epoch [372/3000], Train Loss: 0.1595\n",
      "Epoch [373/3000],jacquard_loss:0.6736842105263158, hammingloss: 0.0727, Precision: 0.8348, Recall: 0.7773, F1: 0.8050\n",
      "Epoch [373/3000], Train Loss: 0.1576\n",
      "Epoch [374/3000],jacquard_loss:0.6785714285714286, hammingloss: 0.0703, Precision: 0.8520, Recall: 0.7692, F1: 0.8085\n",
      "Epoch [374/3000], Train Loss: 0.1575\n",
      "Epoch [375/3000],jacquard_loss:0.6654929577464789, hammingloss: 0.0742, Precision: 0.8363, Recall: 0.7652, F1: 0.7992\n",
      "Epoch [375/3000], Train Loss: 0.1562\n",
      "Epoch [376/3000],jacquard_loss:0.6890459363957597, hammingloss: 0.0688, Precision: 0.8442, Recall: 0.7895, F1: 0.8159\n",
      "Epoch [376/3000], Train Loss: 0.1552\n",
      "Epoch [377/3000],jacquard_loss:0.697508896797153, hammingloss: 0.0664, Precision: 0.8522, Recall: 0.7935, F1: 0.8218\n",
      "Epoch [377/3000], Train Loss: 0.1544\n",
      "Epoch [378/3000],jacquard_loss:0.697841726618705, hammingloss: 0.0656, Precision: 0.8622, Recall: 0.7854, F1: 0.8220\n",
      "Epoch [378/3000], Train Loss: 0.1530\n",
      "Epoch [379/3000],jacquard_loss:0.697508896797153, hammingloss: 0.0664, Precision: 0.8522, Recall: 0.7935, F1: 0.8218\n",
      "Epoch [379/3000], Train Loss: 0.1515\n",
      "Epoch [380/3000],jacquard_loss:0.7025089605734767, hammingloss: 0.0648, Precision: 0.8596, Recall: 0.7935, F1: 0.8253\n",
      "Epoch [380/3000], Train Loss: 0.1510\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [381/3000],jacquard_loss:0.7122302158273381, hammingloss: 0.0625, Precision: 0.8646, Recall: 0.8016, F1: 0.8319\n",
      "Epoch [381/3000], Train Loss: 0.1498\n",
      "Epoch [382/3000],jacquard_loss:0.7184115523465704, hammingloss: 0.0609, Precision: 0.8690, Recall: 0.8057, F1: 0.8361\n",
      "Epoch [382/3000], Train Loss: 0.1490\n",
      "Epoch [383/3000],jacquard_loss:0.7188612099644128, hammingloss: 0.0617, Precision: 0.8559, Recall: 0.8178, F1: 0.8364\n",
      "Epoch [383/3000], Train Loss: 0.1483\n",
      "Epoch [384/3000],jacquard_loss:0.7188612099644128, hammingloss: 0.0617, Precision: 0.8559, Recall: 0.8178, F1: 0.8364\n",
      "Epoch [384/3000], Train Loss: 0.1473\n",
      "Epoch [385/3000],jacquard_loss:0.7292418772563177, hammingloss: 0.0586, Precision: 0.8707, Recall: 0.8178, F1: 0.8434\n",
      "Epoch [385/3000], Train Loss: 0.1466\n",
      "Epoch [386/3000],jacquard_loss:0.7236363636363636, hammingloss: 0.0594, Precision: 0.8767, Recall: 0.8057, F1: 0.8397\n",
      "Epoch [386/3000], Train Loss: 0.1460\n",
      "Epoch [387/3000],jacquard_loss:0.7214285714285714, hammingloss: 0.0609, Precision: 0.8596, Recall: 0.8178, F1: 0.8382\n",
      "Epoch [387/3000], Train Loss: 0.1446\n",
      "Epoch [388/3000],jacquard_loss:0.7163120567375887, hammingloss: 0.0625, Precision: 0.8523, Recall: 0.8178, F1: 0.8347\n",
      "Epoch [388/3000], Train Loss: 0.1441\n",
      "Epoch [389/3000],jacquard_loss:0.7445255474452555, hammingloss: 0.0547, Precision: 0.8831, Recall: 0.8259, F1: 0.8536\n",
      "Epoch [389/3000], Train Loss: 0.1437\n",
      "Epoch [390/3000],jacquard_loss:0.7446043165467626, hammingloss: 0.0555, Precision: 0.8697, Recall: 0.8381, F1: 0.8536\n",
      "Epoch [390/3000], Train Loss: 0.1424\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [391/3000],jacquard_loss:0.7321428571428571, hammingloss: 0.0586, Precision: 0.8613, Recall: 0.8300, F1: 0.8454\n",
      "Epoch [391/3000], Train Loss: 0.1408\n",
      "Epoch [392/3000],jacquard_loss:0.7383512544802867, hammingloss: 0.0570, Precision: 0.8655, Recall: 0.8340, F1: 0.8495\n",
      "Epoch [392/3000], Train Loss: 0.1400\n",
      "Epoch [393/3000],jacquard_loss:0.7490774907749077, hammingloss: 0.0531, Precision: 0.8943, Recall: 0.8219, F1: 0.8565\n",
      "Epoch [393/3000], Train Loss: 0.1389\n",
      "Epoch [394/3000],jacquard_loss:0.7491039426523297, hammingloss: 0.0547, Precision: 0.8672, Recall: 0.8462, F1: 0.8566\n",
      "Epoch [394/3000], Train Loss: 0.1386\n",
      "Epoch [395/3000],jacquard_loss:0.7392857142857143, hammingloss: 0.0570, Precision: 0.8625, Recall: 0.8381, F1: 0.8501\n",
      "Epoch [395/3000], Train Loss: 0.1376\n",
      "Epoch [396/3000],jacquard_loss:0.7343173431734318, hammingloss: 0.0563, Precision: 0.8924, Recall: 0.8057, F1: 0.8468\n",
      "Epoch [396/3000], Train Loss: 0.1369\n",
      "Epoch [397/3000],jacquard_loss:0.7491039426523297, hammingloss: 0.0547, Precision: 0.8672, Recall: 0.8462, F1: 0.8566\n",
      "Epoch [397/3000], Train Loss: 0.1367\n",
      "Epoch [398/3000],jacquard_loss:0.7428571428571429, hammingloss: 0.0563, Precision: 0.8631, Recall: 0.8421, F1: 0.8525\n",
      "Epoch [398/3000], Train Loss: 0.1352\n",
      "Epoch [399/3000],jacquard_loss:0.737037037037037, hammingloss: 0.0555, Precision: 0.8964, Recall: 0.8057, F1: 0.8486\n",
      "Epoch [399/3000], Train Loss: 0.1367\n",
      "Epoch [400/3000],jacquard_loss:0.7554744525547445, hammingloss: 0.0523, Precision: 0.8846, Recall: 0.8381, F1: 0.8607\n",
      "Epoch [400/3000], Train Loss: 0.1340\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [401/3000],jacquard_loss:0.7491039426523297, hammingloss: 0.0547, Precision: 0.8672, Recall: 0.8462, F1: 0.8566\n",
      "Epoch [401/3000], Train Loss: 0.1330\n",
      "Epoch [402/3000],jacquard_loss:0.7527272727272727, hammingloss: 0.0531, Precision: 0.8809, Recall: 0.8381, F1: 0.8589\n",
      "Epoch [402/3000], Train Loss: 0.1326\n",
      "Epoch [403/3000],jacquard_loss:0.7582417582417582, hammingloss: 0.0516, Precision: 0.8884, Recall: 0.8381, F1: 0.8625\n",
      "Epoch [403/3000], Train Loss: 0.1319\n",
      "Epoch [404/3000],jacquard_loss:0.7517985611510791, hammingloss: 0.0539, Precision: 0.8708, Recall: 0.8462, F1: 0.8583\n",
      "Epoch [404/3000], Train Loss: 0.1315\n",
      "Epoch [405/3000],jacquard_loss:0.7455197132616488, hammingloss: 0.0555, Precision: 0.8667, Recall: 0.8421, F1: 0.8542\n",
      "Epoch [405/3000], Train Loss: 0.1325\n",
      "Epoch [406/3000],jacquard_loss:0.7355072463768116, hammingloss: 0.0570, Precision: 0.8750, Recall: 0.8219, F1: 0.8476\n",
      "Epoch [406/3000], Train Loss: 0.1301\n",
      "Epoch [407/3000],jacquard_loss:0.7709090909090909, hammingloss: 0.0492, Precision: 0.8833, Recall: 0.8583, F1: 0.8706\n",
      "Epoch [407/3000], Train Loss: 0.1302\n",
      "Epoch [408/3000],jacquard_loss:0.7481751824817519, hammingloss: 0.0539, Precision: 0.8836, Recall: 0.8300, F1: 0.8559\n",
      "Epoch [408/3000], Train Loss: 0.1279\n",
      "Epoch [409/3000],jacquard_loss:0.7720588235294118, hammingloss: 0.0484, Precision: 0.8936, Recall: 0.8502, F1: 0.8714\n",
      "Epoch [409/3000], Train Loss: 0.1294\n",
      "Epoch [410/3000],jacquard_loss:0.7302158273381295, hammingloss: 0.0586, Precision: 0.8675, Recall: 0.8219, F1: 0.8441\n",
      "Epoch [410/3000], Train Loss: 0.1290\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [411/3000],jacquard_loss:0.7437722419928826, hammingloss: 0.0563, Precision: 0.8601, Recall: 0.8462, F1: 0.8531\n",
      "Epoch [411/3000], Train Loss: 0.1322\n",
      "Epoch [412/3000],jacquard_loss:0.7435897435897436, hammingloss: 0.0547, Precision: 0.8865, Recall: 0.8219, F1: 0.8529\n",
      "Epoch [412/3000], Train Loss: 0.1283\n",
      "Epoch [413/3000],jacquard_loss:0.75177304964539, hammingloss: 0.0547, Precision: 0.8583, Recall: 0.8583, F1: 0.8583\n",
      "Epoch [413/3000], Train Loss: 0.1289\n",
      "Epoch [414/3000],jacquard_loss:0.7518518518518519, hammingloss: 0.0523, Precision: 0.8982, Recall: 0.8219, F1: 0.8584\n",
      "Epoch [414/3000], Train Loss: 0.1254\n",
      "Epoch [415/3000],jacquard_loss:0.7794117647058824, hammingloss: 0.0469, Precision: 0.8945, Recall: 0.8583, F1: 0.8760\n",
      "Epoch [415/3000], Train Loss: 0.1275\n",
      "Epoch [416/3000],jacquard_loss:0.7472527472527473, hammingloss: 0.0539, Precision: 0.8870, Recall: 0.8259, F1: 0.8553\n",
      "Epoch [416/3000], Train Loss: 0.1258\n",
      "Epoch [417/3000],jacquard_loss:0.7419354838709677, hammingloss: 0.0563, Precision: 0.8661, Recall: 0.8381, F1: 0.8519\n",
      "Epoch [417/3000], Train Loss: 0.1294\n",
      "Epoch [418/3000],jacquard_loss:0.7286245353159851, hammingloss: 0.0570, Precision: 0.8991, Recall: 0.7935, F1: 0.8430\n",
      "Epoch [418/3000], Train Loss: 0.1267\n",
      "Epoch [419/3000],jacquard_loss:0.7482517482517482, hammingloss: 0.0563, Precision: 0.8458, Recall: 0.8664, F1: 0.8560\n",
      "Epoch [419/3000], Train Loss: 0.1248\n",
      "Epoch [420/3000],jacquard_loss:0.7453183520599251, hammingloss: 0.0531, Precision: 0.9087, Recall: 0.8057, F1: 0.8541\n",
      "Epoch [420/3000], Train Loss: 0.1228\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [421/3000],jacquard_loss:0.7651245551601423, hammingloss: 0.0516, Precision: 0.8635, Recall: 0.8704, F1: 0.8669\n",
      "Epoch [421/3000], Train Loss: 0.1217\n",
      "Epoch [422/3000],jacquard_loss:0.7649253731343284, hammingloss: 0.0492, Precision: 0.9071, Recall: 0.8300, F1: 0.8668\n",
      "Epoch [422/3000], Train Loss: 0.1193\n",
      "Epoch [423/3000],jacquard_loss:0.7686832740213523, hammingloss: 0.0508, Precision: 0.8640, Recall: 0.8745, F1: 0.8692\n",
      "Epoch [423/3000], Train Loss: 0.1183\n",
      "Epoch [424/3000],jacquard_loss:0.7819548872180451, hammingloss: 0.0453, Precision: 0.9163, Recall: 0.8421, F1: 0.8776\n",
      "Epoch [424/3000], Train Loss: 0.1168\n",
      "Epoch [425/3000],jacquard_loss:0.7854545454545454, hammingloss: 0.0461, Precision: 0.8852, Recall: 0.8745, F1: 0.8798\n",
      "Epoch [425/3000], Train Loss: 0.1161\n",
      "Epoch [426/3000],jacquard_loss:0.7814814814814814, hammingloss: 0.0461, Precision: 0.9017, Recall: 0.8543, F1: 0.8773\n",
      "Epoch [426/3000], Train Loss: 0.1163\n",
      "Epoch [427/3000],jacquard_loss:0.7843866171003717, hammingloss: 0.0453, Precision: 0.9056, Recall: 0.8543, F1: 0.8792\n",
      "Epoch [427/3000], Train Loss: 0.1150\n",
      "Epoch [428/3000],jacquard_loss:0.7859778597785978, hammingloss: 0.0453, Precision: 0.8987, Recall: 0.8623, F1: 0.8802\n",
      "Epoch [428/3000], Train Loss: 0.1138\n",
      "Epoch [429/3000],jacquard_loss:0.7851851851851852, hammingloss: 0.0453, Precision: 0.9021, Recall: 0.8583, F1: 0.8797\n",
      "Epoch [429/3000], Train Loss: 0.1145\n",
      "Epoch [430/3000],jacquard_loss:0.774074074074074, hammingloss: 0.0477, Precision: 0.9009, Recall: 0.8462, F1: 0.8727\n",
      "Epoch [430/3000], Train Loss: 0.1134\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [431/3000],jacquard_loss:0.7888888888888889, hammingloss: 0.0445, Precision: 0.9025, Recall: 0.8623, F1: 0.8820\n",
      "Epoch [431/3000], Train Loss: 0.1117\n",
      "Epoch [432/3000],jacquard_loss:0.7814814814814814, hammingloss: 0.0461, Precision: 0.9017, Recall: 0.8543, F1: 0.8773\n",
      "Epoch [432/3000], Train Loss: 0.1130\n",
      "Epoch [433/3000],jacquard_loss:0.7859778597785978, hammingloss: 0.0453, Precision: 0.8987, Recall: 0.8623, F1: 0.8802\n",
      "Epoch [433/3000], Train Loss: 0.1129\n",
      "Epoch [434/3000],jacquard_loss:0.7947761194029851, hammingloss: 0.0430, Precision: 0.9103, Recall: 0.8623, F1: 0.8857\n",
      "Epoch [434/3000], Train Loss: 0.1105\n",
      "Epoch [435/3000],jacquard_loss:0.7969924812030075, hammingloss: 0.0422, Precision: 0.9177, Recall: 0.8583, F1: 0.8870\n",
      "Epoch [435/3000], Train Loss: 0.1112\n",
      "Epoch [436/3000],jacquard_loss:0.7838827838827839, hammingloss: 0.0461, Precision: 0.8917, Recall: 0.8664, F1: 0.8789\n",
      "Epoch [436/3000], Train Loss: 0.1120\n",
      "Epoch [437/3000],jacquard_loss:0.7644927536231884, hammingloss: 0.0508, Precision: 0.8792, Recall: 0.8543, F1: 0.8665\n",
      "Epoch [437/3000], Train Loss: 0.1117\n",
      "Epoch [438/3000],jacquard_loss:0.7894736842105263, hammingloss: 0.0437, Precision: 0.9170, Recall: 0.8502, F1: 0.8824\n",
      "Epoch [438/3000], Train Loss: 0.1100\n",
      "Epoch [439/3000],jacquard_loss:0.8097014925373134, hammingloss: 0.0398, Precision: 0.9118, Recall: 0.8785, F1: 0.8948\n",
      "Epoch [439/3000], Train Loss: 0.1069\n",
      "Epoch [440/3000],jacquard_loss:0.8106060606060606, hammingloss: 0.0391, Precision: 0.9264, Recall: 0.8664, F1: 0.8954\n",
      "Epoch [440/3000], Train Loss: 0.1063\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [441/3000],jacquard_loss:0.7941176470588235, hammingloss: 0.0437, Precision: 0.8963, Recall: 0.8745, F1: 0.8852\n",
      "Epoch [441/3000], Train Loss: 0.1059\n",
      "Epoch [442/3000],jacquard_loss:0.8037735849056604, hammingloss: 0.0406, Precision: 0.9221, Recall: 0.8623, F1: 0.8912\n",
      "Epoch [442/3000], Train Loss: 0.1040\n",
      "Epoch [443/3000],jacquard_loss:0.8059701492537313, hammingloss: 0.0406, Precision: 0.9114, Recall: 0.8745, F1: 0.8926\n",
      "Epoch [443/3000], Train Loss: 0.1044\n",
      "Epoch [444/3000],jacquard_loss:0.8045112781954887, hammingloss: 0.0406, Precision: 0.9185, Recall: 0.8664, F1: 0.8917\n",
      "Epoch [444/3000], Train Loss: 0.1034\n",
      "Epoch [445/3000],jacquard_loss:0.8150943396226416, hammingloss: 0.0383, Precision: 0.9231, Recall: 0.8745, F1: 0.8981\n",
      "Epoch [445/3000], Train Loss: 0.1022\n",
      "Epoch [446/3000],jacquard_loss:0.8106060606060606, hammingloss: 0.0391, Precision: 0.9264, Recall: 0.8664, F1: 0.8954\n",
      "Epoch [446/3000], Train Loss: 0.1026\n",
      "Epoch [447/3000],jacquard_loss:0.8089887640449438, hammingloss: 0.0398, Precision: 0.9153, Recall: 0.8745, F1: 0.8944\n",
      "Epoch [447/3000], Train Loss: 0.1019\n",
      "Epoch [448/3000],jacquard_loss:0.8181818181818182, hammingloss: 0.0375, Precision: 0.9270, Recall: 0.8745, F1: 0.9000\n",
      "Epoch [448/3000], Train Loss: 0.1005\n",
      "Epoch [449/3000],jacquard_loss:0.8059701492537313, hammingloss: 0.0406, Precision: 0.9114, Recall: 0.8745, F1: 0.8926\n",
      "Epoch [449/3000], Train Loss: 0.1013\n",
      "Epoch [450/3000],jacquard_loss:0.8098859315589354, hammingloss: 0.0391, Precision: 0.9301, Recall: 0.8623, F1: 0.8950\n",
      "Epoch [450/3000], Train Loss: 0.0999\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [451/3000],jacquard_loss:0.8029739776951673, hammingloss: 0.0414, Precision: 0.9076, Recall: 0.8745, F1: 0.8907\n",
      "Epoch [451/3000], Train Loss: 0.0999\n",
      "Epoch [452/3000],jacquard_loss:0.8015267175572519, hammingloss: 0.0406, Precision: 0.9333, Recall: 0.8502, F1: 0.8898\n",
      "Epoch [452/3000], Train Loss: 0.1001\n",
      "Epoch [453/3000],jacquard_loss:0.8022388059701493, hammingloss: 0.0414, Precision: 0.9110, Recall: 0.8704, F1: 0.8903\n",
      "Epoch [453/3000], Train Loss: 0.0991\n",
      "Epoch [454/3000],jacquard_loss:0.7946768060836502, hammingloss: 0.0422, Precision: 0.9289, Recall: 0.8462, F1: 0.8856\n",
      "Epoch [454/3000], Train Loss: 0.0984\n",
      "Epoch [455/3000],jacquard_loss:0.8044280442804428, hammingloss: 0.0414, Precision: 0.9008, Recall: 0.8826, F1: 0.8916\n",
      "Epoch [455/3000], Train Loss: 0.0994\n",
      "Epoch [456/3000],jacquard_loss:0.7984790874524715, hammingloss: 0.0414, Precision: 0.9292, Recall: 0.8502, F1: 0.8879\n",
      "Epoch [456/3000], Train Loss: 0.0966\n",
      "Epoch [457/3000],jacquard_loss:0.8089887640449438, hammingloss: 0.0398, Precision: 0.9153, Recall: 0.8745, F1: 0.8944\n",
      "Epoch [457/3000], Train Loss: 0.0978\n",
      "Epoch [458/3000],jacquard_loss:0.7984790874524715, hammingloss: 0.0414, Precision: 0.9292, Recall: 0.8502, F1: 0.8879\n",
      "Epoch [458/3000], Train Loss: 0.0984\n",
      "Epoch [459/3000],jacquard_loss:0.8052434456928839, hammingloss: 0.0406, Precision: 0.9149, Recall: 0.8704, F1: 0.8921\n",
      "Epoch [459/3000], Train Loss: 0.0978\n",
      "Epoch [460/3000],jacquard_loss:0.7984790874524715, hammingloss: 0.0414, Precision: 0.9292, Recall: 0.8502, F1: 0.8879\n",
      "Epoch [460/3000], Train Loss: 0.0958\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [461/3000],jacquard_loss:0.8118081180811808, hammingloss: 0.0398, Precision: 0.9016, Recall: 0.8907, F1: 0.8961\n",
      "Epoch [461/3000], Train Loss: 0.1003\n",
      "Epoch [462/3000],jacquard_loss:0.8, hammingloss: 0.0406, Precision: 0.9412, Recall: 0.8421, F1: 0.8889\n",
      "Epoch [462/3000], Train Loss: 0.0954\n",
      "Epoch [463/3000],jacquard_loss:0.8066914498141264, hammingloss: 0.0406, Precision: 0.9079, Recall: 0.8785, F1: 0.8930\n",
      "Epoch [463/3000], Train Loss: 0.0971\n",
      "Epoch [464/3000],jacquard_loss:0.8192307692307692, hammingloss: 0.0367, Precision: 0.9425, Recall: 0.8623, F1: 0.9006\n",
      "Epoch [464/3000], Train Loss: 0.0961\n",
      "Epoch [465/3000],jacquard_loss:0.7948717948717948, hammingloss: 0.0437, Precision: 0.8930, Recall: 0.8785, F1: 0.8857\n",
      "Epoch [465/3000], Train Loss: 0.0979\n",
      "Epoch [466/3000],jacquard_loss:0.7992424242424242, hammingloss: 0.0414, Precision: 0.9254, Recall: 0.8543, F1: 0.8884\n",
      "Epoch [466/3000], Train Loss: 0.0940\n",
      "Epoch [467/3000],jacquard_loss:0.8154981549815498, hammingloss: 0.0391, Precision: 0.9020, Recall: 0.8947, F1: 0.8984\n",
      "Epoch [467/3000], Train Loss: 0.0982\n",
      "Epoch [468/3000],jacquard_loss:0.8153846153846154, hammingloss: 0.0375, Precision: 0.9422, Recall: 0.8583, F1: 0.8983\n",
      "Epoch [468/3000], Train Loss: 0.0931\n",
      "Epoch [469/3000],jacquard_loss:0.8246268656716418, hammingloss: 0.0367, Precision: 0.9132, Recall: 0.8947, F1: 0.9039\n",
      "Epoch [469/3000], Train Loss: 0.0943\n",
      "Epoch [470/3000],jacquard_loss:0.8294573643410853, hammingloss: 0.0344, Precision: 0.9511, Recall: 0.8664, F1: 0.9068\n",
      "Epoch [470/3000], Train Loss: 0.0902\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [471/3000],jacquard_loss:0.8178438661710037, hammingloss: 0.0383, Precision: 0.9091, Recall: 0.8907, F1: 0.8998\n",
      "Epoch [471/3000], Train Loss: 0.0916\n",
      "Epoch [472/3000],jacquard_loss:0.8206106870229007, hammingloss: 0.0367, Precision: 0.9348, Recall: 0.8704, F1: 0.9015\n",
      "Epoch [472/3000], Train Loss: 0.0878\n",
      "Epoch [473/3000],jacquard_loss:0.8277153558052435, hammingloss: 0.0359, Precision: 0.9170, Recall: 0.8947, F1: 0.9057\n",
      "Epoch [473/3000], Train Loss: 0.0892\n",
      "Epoch [474/3000],jacquard_loss:0.8372093023255814, hammingloss: 0.0328, Precision: 0.9515, Recall: 0.8745, F1: 0.9114\n",
      "Epoch [474/3000], Train Loss: 0.0867\n",
      "Epoch [475/3000],jacquard_loss:0.8345864661654135, hammingloss: 0.0344, Precision: 0.9212, Recall: 0.8988, F1: 0.9098\n",
      "Epoch [475/3000], Train Loss: 0.0863\n",
      "Epoch [476/3000],jacquard_loss:0.8390804597701149, hammingloss: 0.0328, Precision: 0.9399, Recall: 0.8866, F1: 0.9125\n",
      "Epoch [476/3000], Train Loss: 0.0850\n",
      "Epoch [477/3000],jacquard_loss:0.8423076923076923, hammingloss: 0.0320, Precision: 0.9440, Recall: 0.8866, F1: 0.9144\n",
      "Epoch [477/3000], Train Loss: 0.0842\n",
      "Epoch [478/3000],jacquard_loss:0.8435114503816794, hammingloss: 0.0320, Precision: 0.9364, Recall: 0.8947, F1: 0.9151\n",
      "Epoch [478/3000], Train Loss: 0.0831\n",
      "Epoch [479/3000],jacquard_loss:0.844106463878327, hammingloss: 0.0320, Precision: 0.9328, Recall: 0.8988, F1: 0.9155\n",
      "Epoch [479/3000], Train Loss: 0.0848\n",
      "Epoch [480/3000],jacquard_loss:0.8565891472868217, hammingloss: 0.0289, Precision: 0.9526, Recall: 0.8947, F1: 0.9228\n",
      "Epoch [480/3000], Train Loss: 0.0820\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [481/3000],jacquard_loss:0.8544061302681992, hammingloss: 0.0297, Precision: 0.9409, Recall: 0.9028, F1: 0.9215\n",
      "Epoch [481/3000], Train Loss: 0.0815\n",
      "Epoch [482/3000],jacquard_loss:0.85, hammingloss: 0.0305, Precision: 0.9444, Recall: 0.8947, F1: 0.9189\n",
      "Epoch [482/3000], Train Loss: 0.0806\n",
      "Epoch [483/3000],jacquard_loss:0.8467432950191571, hammingloss: 0.0312, Precision: 0.9404, Recall: 0.8947, F1: 0.9170\n",
      "Epoch [483/3000], Train Loss: 0.0806\n",
      "Epoch [484/3000],jacquard_loss:0.8467432950191571, hammingloss: 0.0312, Precision: 0.9404, Recall: 0.8947, F1: 0.9170\n",
      "Epoch [484/3000], Train Loss: 0.0796\n",
      "Epoch [485/3000],jacquard_loss:0.8538461538461538, hammingloss: 0.0297, Precision: 0.9447, Recall: 0.8988, F1: 0.9212\n",
      "Epoch [485/3000], Train Loss: 0.0814\n",
      "Epoch [486/3000],jacquard_loss:0.85, hammingloss: 0.0305, Precision: 0.9444, Recall: 0.8947, F1: 0.9189\n",
      "Epoch [486/3000], Train Loss: 0.0795\n",
      "Epoch [487/3000],jacquard_loss:0.851145038167939, hammingloss: 0.0305, Precision: 0.9370, Recall: 0.9028, F1: 0.9196\n",
      "Epoch [487/3000], Train Loss: 0.0802\n",
      "Epoch [488/3000],jacquard_loss:0.8275862068965517, hammingloss: 0.0352, Precision: 0.9391, Recall: 0.8745, F1: 0.9057\n",
      "Epoch [488/3000], Train Loss: 0.0788\n",
      "Epoch [489/3000],jacquard_loss:0.851145038167939, hammingloss: 0.0305, Precision: 0.9370, Recall: 0.9028, F1: 0.9196\n",
      "Epoch [489/3000], Train Loss: 0.0810\n",
      "Epoch [490/3000],jacquard_loss:0.8257575757575758, hammingloss: 0.0359, Precision: 0.9277, Recall: 0.8826, F1: 0.9046\n",
      "Epoch [490/3000], Train Loss: 0.0797\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [491/3000],jacquard_loss:0.8484848484848485, hammingloss: 0.0312, Precision: 0.9295, Recall: 0.9069, F1: 0.9180\n",
      "Epoch [491/3000], Train Loss: 0.0818\n",
      "Epoch [492/3000],jacquard_loss:0.8443579766536965, hammingloss: 0.0312, Precision: 0.9559, Recall: 0.8785, F1: 0.9156\n",
      "Epoch [492/3000], Train Loss: 0.0804\n",
      "Epoch [493/3000],jacquard_loss:0.8458646616541353, hammingloss: 0.0320, Precision: 0.9221, Recall: 0.9109, F1: 0.9165\n",
      "Epoch [493/3000], Train Loss: 0.0810\n",
      "Epoch [494/3000],jacquard_loss:0.8244274809160306, hammingloss: 0.0359, Precision: 0.9351, Recall: 0.8745, F1: 0.9038\n",
      "Epoch [494/3000], Train Loss: 0.0777\n",
      "Epoch [495/3000],jacquard_loss:0.8517110266159695, hammingloss: 0.0305, Precision: 0.9333, Recall: 0.9069, F1: 0.9199\n",
      "Epoch [495/3000], Train Loss: 0.0809\n",
      "Epoch [496/3000],jacquard_loss:0.8199233716475096, hammingloss: 0.0367, Precision: 0.9386, Recall: 0.8664, F1: 0.9011\n",
      "Epoch [496/3000], Train Loss: 0.0834\n",
      "Epoch [497/3000],jacquard_loss:0.8339483394833949, hammingloss: 0.0352, Precision: 0.9040, Recall: 0.9150, F1: 0.9095\n",
      "Epoch [497/3000], Train Loss: 0.0836\n",
      "Epoch [498/3000],jacquard_loss:0.8192307692307692, hammingloss: 0.0367, Precision: 0.9425, Recall: 0.8623, F1: 0.9006\n",
      "Epoch [498/3000], Train Loss: 0.0798\n",
      "Epoch [499/3000],jacquard_loss:0.8560606060606061, hammingloss: 0.0297, Precision: 0.9300, Recall: 0.9150, F1: 0.9224\n",
      "Epoch [499/3000], Train Loss: 0.0829\n",
      "Epoch [500/3000],jacquard_loss:0.7977099236641222, hammingloss: 0.0414, Precision: 0.9330, Recall: 0.8462, F1: 0.8875\n",
      "Epoch [500/3000], Train Loss: 0.0867\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [501/3000],jacquard_loss:0.8438661710037175, hammingloss: 0.0328, Precision: 0.9116, Recall: 0.9190, F1: 0.9153\n",
      "Epoch [501/3000], Train Loss: 0.0826\n",
      "Epoch [502/3000],jacquard_loss:0.8129770992366412, hammingloss: 0.0383, Precision: 0.9342, Recall: 0.8623, F1: 0.8968\n",
      "Epoch [502/3000], Train Loss: 0.0850\n",
      "Epoch [503/3000],jacquard_loss:0.828996282527881, hammingloss: 0.0359, Precision: 0.9102, Recall: 0.9028, F1: 0.9065\n",
      "Epoch [503/3000], Train Loss: 0.0831\n",
      "Epoch [504/3000],jacquard_loss:0.8185328185328186, hammingloss: 0.0367, Precision: 0.9464, Recall: 0.8583, F1: 0.9002\n",
      "Epoch [504/3000], Train Loss: 0.0854\n",
      "Epoch [505/3000],jacquard_loss:0.8388278388278388, hammingloss: 0.0344, Precision: 0.8980, Recall: 0.9271, F1: 0.9124\n",
      "Epoch [505/3000], Train Loss: 0.0778\n",
      "Epoch [506/3000],jacquard_loss:0.8565891472868217, hammingloss: 0.0289, Precision: 0.9526, Recall: 0.8947, F1: 0.9228\n",
      "Epoch [506/3000], Train Loss: 0.0789\n",
      "Epoch [507/3000],jacquard_loss:0.8333333333333334, hammingloss: 0.0352, Precision: 0.9073, Recall: 0.9109, F1: 0.9091\n",
      "Epoch [507/3000], Train Loss: 0.0771\n",
      "Epoch [508/3000],jacquard_loss:0.8403041825095057, hammingloss: 0.0328, Precision: 0.9325, Recall: 0.8947, F1: 0.9132\n",
      "Epoch [508/3000], Train Loss: 0.0798\n",
      "Epoch [509/3000],jacquard_loss:0.8415094339622642, hammingloss: 0.0328, Precision: 0.9253, Recall: 0.9028, F1: 0.9139\n",
      "Epoch [509/3000], Train Loss: 0.0720\n",
      "Epoch [510/3000],jacquard_loss:0.8461538461538461, hammingloss: 0.0312, Precision: 0.9442, Recall: 0.8907, F1: 0.9167\n",
      "Epoch [510/3000], Train Loss: 0.0793\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [511/3000],jacquard_loss:0.8389513108614233, hammingloss: 0.0336, Precision: 0.9180, Recall: 0.9069, F1: 0.9124\n",
      "Epoch [511/3000], Train Loss: 0.0746\n",
      "Epoch [512/3000],jacquard_loss:0.8371212121212122, hammingloss: 0.0336, Precision: 0.9286, Recall: 0.8947, F1: 0.9113\n",
      "Epoch [512/3000], Train Loss: 0.0787\n",
      "Epoch [513/3000],jacquard_loss:0.8549618320610687, hammingloss: 0.0297, Precision: 0.9372, Recall: 0.9069, F1: 0.9218\n",
      "Epoch [513/3000], Train Loss: 0.0714\n",
      "Epoch [514/3000],jacquard_loss:0.8615384615384616, hammingloss: 0.0281, Precision: 0.9451, Recall: 0.9069, F1: 0.9256\n",
      "Epoch [514/3000], Train Loss: 0.0740\n",
      "Epoch [515/3000],jacquard_loss:0.8620689655172413, hammingloss: 0.0281, Precision: 0.9414, Recall: 0.9109, F1: 0.9259\n",
      "Epoch [515/3000], Train Loss: 0.0684\n",
      "Epoch [516/3000],jacquard_loss:0.8648648648648649, hammingloss: 0.0273, Precision: 0.9492, Recall: 0.9069, F1: 0.9275\n",
      "Epoch [516/3000], Train Loss: 0.0676\n",
      "Epoch [517/3000],jacquard_loss:0.8725868725868726, hammingloss: 0.0258, Precision: 0.9496, Recall: 0.9150, F1: 0.9320\n",
      "Epoch [517/3000], Train Loss: 0.0657\n",
      "Epoch [518/3000],jacquard_loss:0.875968992248062, hammingloss: 0.0250, Precision: 0.9536, Recall: 0.9150, F1: 0.9339\n",
      "Epoch [518/3000], Train Loss: 0.0640\n",
      "Epoch [519/3000],jacquard_loss:0.8832684824902723, hammingloss: 0.0234, Precision: 0.9578, Recall: 0.9190, F1: 0.9380\n",
      "Epoch [519/3000], Train Loss: 0.0650\n",
      "Epoch [520/3000],jacquard_loss:0.8730769230769231, hammingloss: 0.0258, Precision: 0.9458, Recall: 0.9190, F1: 0.9322\n",
      "Epoch [520/3000], Train Loss: 0.0633\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [521/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [521/3000], Train Loss: 0.0648\n",
      "Epoch [522/3000],jacquard_loss:0.8725868725868726, hammingloss: 0.0258, Precision: 0.9496, Recall: 0.9150, F1: 0.9320\n",
      "Epoch [522/3000], Train Loss: 0.0624\n",
      "Epoch [523/3000],jacquard_loss:0.8875968992248062, hammingloss: 0.0227, Precision: 0.9542, Recall: 0.9271, F1: 0.9405\n",
      "Epoch [523/3000], Train Loss: 0.0629\n",
      "Epoch [524/3000],jacquard_loss:0.8793774319066148, hammingloss: 0.0242, Precision: 0.9576, Recall: 0.9150, F1: 0.9358\n",
      "Epoch [524/3000], Train Loss: 0.0607\n",
      "Epoch [525/3000],jacquard_loss:0.8914728682170543, hammingloss: 0.0219, Precision: 0.9544, Recall: 0.9312, F1: 0.9426\n",
      "Epoch [525/3000], Train Loss: 0.0611\n",
      "Epoch [526/3000],jacquard_loss:0.88671875, hammingloss: 0.0227, Precision: 0.9619, Recall: 0.9190, F1: 0.9400\n",
      "Epoch [526/3000], Train Loss: 0.0608\n",
      "Epoch [527/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [527/3000], Train Loss: 0.0610\n",
      "Epoch [528/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [528/3000], Train Loss: 0.0597\n",
      "Epoch [529/3000],jacquard_loss:0.8875968992248062, hammingloss: 0.0227, Precision: 0.9542, Recall: 0.9271, F1: 0.9405\n",
      "Epoch [529/3000], Train Loss: 0.0625\n",
      "Epoch [530/3000],jacquard_loss:0.8871595330739299, hammingloss: 0.0227, Precision: 0.9580, Recall: 0.9231, F1: 0.9402\n",
      "Epoch [530/3000], Train Loss: 0.0597\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [531/3000],jacquard_loss:0.89453125, hammingloss: 0.0211, Precision: 0.9622, Recall: 0.9271, F1: 0.9443\n",
      "Epoch [531/3000], Train Loss: 0.0597\n",
      "Epoch [532/3000],jacquard_loss:0.8901960784313725, hammingloss: 0.0219, Precision: 0.9660, Recall: 0.9190, F1: 0.9419\n",
      "Epoch [532/3000], Train Loss: 0.0583\n",
      "Epoch [533/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [533/3000], Train Loss: 0.0604\n",
      "Epoch [534/3000],jacquard_loss:0.90234375, hammingloss: 0.0195, Precision: 0.9625, Recall: 0.9352, F1: 0.9487\n",
      "Epoch [534/3000], Train Loss: 0.0571\n",
      "Epoch [535/3000],jacquard_loss:0.8871595330739299, hammingloss: 0.0227, Precision: 0.9580, Recall: 0.9231, F1: 0.9402\n",
      "Epoch [535/3000], Train Loss: 0.0605\n",
      "Epoch [536/3000],jacquard_loss:0.8664122137404581, hammingloss: 0.0273, Precision: 0.9380, Recall: 0.9190, F1: 0.9284\n",
      "Epoch [536/3000], Train Loss: 0.0603\n",
      "Epoch [537/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [537/3000], Train Loss: 0.0609\n",
      "Epoch [538/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [538/3000], Train Loss: 0.0566\n",
      "Epoch [539/3000],jacquard_loss:0.8764478764478765, hammingloss: 0.0250, Precision: 0.9498, Recall: 0.9190, F1: 0.9342\n",
      "Epoch [539/3000], Train Loss: 0.0605\n",
      "Epoch [540/3000],jacquard_loss:0.8837209302325582, hammingloss: 0.0234, Precision: 0.9540, Recall: 0.9231, F1: 0.9383\n",
      "Epoch [540/3000], Train Loss: 0.0601\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [541/3000],jacquard_loss:0.8764478764478765, hammingloss: 0.0250, Precision: 0.9498, Recall: 0.9190, F1: 0.9342\n",
      "Epoch [541/3000], Train Loss: 0.0649\n",
      "Epoch [542/3000],jacquard_loss:0.8884615384615384, hammingloss: 0.0227, Precision: 0.9467, Recall: 0.9352, F1: 0.9409\n",
      "Epoch [542/3000], Train Loss: 0.0586\n",
      "Epoch [543/3000],jacquard_loss:0.8692307692307693, hammingloss: 0.0266, Precision: 0.9456, Recall: 0.9150, F1: 0.9300\n",
      "Epoch [543/3000], Train Loss: 0.0662\n",
      "Epoch [544/3000],jacquard_loss:0.8646616541353384, hammingloss: 0.0281, Precision: 0.9237, Recall: 0.9312, F1: 0.9274\n",
      "Epoch [544/3000], Train Loss: 0.0649\n",
      "Epoch [545/3000],jacquard_loss:0.844106463878327, hammingloss: 0.0320, Precision: 0.9328, Recall: 0.8988, F1: 0.9155\n",
      "Epoch [545/3000], Train Loss: 0.0710\n",
      "Epoch [546/3000],jacquard_loss:0.8528301886792453, hammingloss: 0.0305, Precision: 0.9262, Recall: 0.9150, F1: 0.9206\n",
      "Epoch [546/3000], Train Loss: 0.0636\n",
      "Epoch [547/3000],jacquard_loss:0.8473282442748091, hammingloss: 0.0312, Precision: 0.9367, Recall: 0.8988, F1: 0.9174\n",
      "Epoch [547/3000], Train Loss: 0.0714\n",
      "Epoch [548/3000],jacquard_loss:0.850187265917603, hammingloss: 0.0312, Precision: 0.9190, Recall: 0.9190, F1: 0.9190\n",
      "Epoch [548/3000], Train Loss: 0.0636\n",
      "Epoch [549/3000],jacquard_loss:0.8409090909090909, hammingloss: 0.0328, Precision: 0.9289, Recall: 0.8988, F1: 0.9136\n",
      "Epoch [549/3000], Train Loss: 0.0706\n",
      "Epoch [550/3000],jacquard_loss:0.8721804511278195, hammingloss: 0.0266, Precision: 0.9243, Recall: 0.9393, F1: 0.9317\n",
      "Epoch [550/3000], Train Loss: 0.0625\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [551/3000],jacquard_loss:0.8648648648648649, hammingloss: 0.0273, Precision: 0.9492, Recall: 0.9069, F1: 0.9275\n",
      "Epoch [551/3000], Train Loss: 0.0611\n",
      "Epoch [552/3000],jacquard_loss:0.8949416342412452, hammingloss: 0.0211, Precision: 0.9583, Recall: 0.9312, F1: 0.9446\n",
      "Epoch [552/3000], Train Loss: 0.0573\n",
      "Epoch [553/3000],jacquard_loss:0.8996138996138996, hammingloss: 0.0203, Precision: 0.9510, Recall: 0.9433, F1: 0.9472\n",
      "Epoch [553/3000], Train Loss: 0.0534\n",
      "Epoch [554/3000],jacquard_loss:0.8910505836575876, hammingloss: 0.0219, Precision: 0.9582, Recall: 0.9271, F1: 0.9424\n",
      "Epoch [554/3000], Train Loss: 0.0562\n",
      "Epoch [555/3000],jacquard_loss:0.90625, hammingloss: 0.0187, Precision: 0.9627, Recall: 0.9393, F1: 0.9508\n",
      "Epoch [555/3000], Train Loss: 0.0531\n",
      "Epoch [556/3000],jacquard_loss:0.8984375, hammingloss: 0.0203, Precision: 0.9623, Recall: 0.9312, F1: 0.9465\n",
      "Epoch [556/3000], Train Loss: 0.0528\n",
      "Epoch [557/3000],jacquard_loss:0.9015748031496063, hammingloss: 0.0195, Precision: 0.9703, Recall: 0.9271, F1: 0.9482\n",
      "Epoch [557/3000], Train Loss: 0.0528\n",
      "Epoch [558/3000],jacquard_loss:0.8996138996138996, hammingloss: 0.0203, Precision: 0.9510, Recall: 0.9433, F1: 0.9472\n",
      "Epoch [558/3000], Train Loss: 0.0514\n",
      "Epoch [559/3000],jacquard_loss:0.9051383399209486, hammingloss: 0.0187, Precision: 0.9745, Recall: 0.9271, F1: 0.9502\n",
      "Epoch [559/3000], Train Loss: 0.0506\n",
      "Epoch [560/3000],jacquard_loss:0.91796875, hammingloss: 0.0164, Precision: 0.9631, Recall: 0.9514, F1: 0.9572\n",
      "Epoch [560/3000], Train Loss: 0.0492\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [561/3000],jacquard_loss:0.91015625, hammingloss: 0.0180, Precision: 0.9628, Recall: 0.9433, F1: 0.9530\n",
      "Epoch [561/3000], Train Loss: 0.0501\n",
      "Epoch [562/3000],jacquard_loss:0.9212598425196851, hammingloss: 0.0156, Precision: 0.9710, Recall: 0.9474, F1: 0.9590\n",
      "Epoch [562/3000], Train Loss: 0.0480\n",
      "Epoch [563/3000],jacquard_loss:0.9105058365758755, hammingloss: 0.0180, Precision: 0.9590, Recall: 0.9474, F1: 0.9532\n",
      "Epoch [563/3000], Train Loss: 0.0478\n",
      "Epoch [564/3000],jacquard_loss:0.924901185770751, hammingloss: 0.0148, Precision: 0.9750, Recall: 0.9474, F1: 0.9610\n",
      "Epoch [564/3000], Train Loss: 0.0474\n",
      "Epoch [565/3000],jacquard_loss:0.91796875, hammingloss: 0.0164, Precision: 0.9631, Recall: 0.9514, F1: 0.9572\n",
      "Epoch [565/3000], Train Loss: 0.0466\n",
      "Epoch [566/3000],jacquard_loss:0.9140625, hammingloss: 0.0172, Precision: 0.9630, Recall: 0.9474, F1: 0.9551\n",
      "Epoch [566/3000], Train Loss: 0.0478\n",
      "Epoch [567/3000],jacquard_loss:0.924901185770751, hammingloss: 0.0148, Precision: 0.9750, Recall: 0.9474, F1: 0.9610\n",
      "Epoch [567/3000], Train Loss: 0.0484\n",
      "Epoch [568/3000],jacquard_loss:0.9108527131782945, hammingloss: 0.0180, Precision: 0.9553, Recall: 0.9514, F1: 0.9533\n",
      "Epoch [568/3000], Train Loss: 0.0466\n",
      "Epoch [569/3000],jacquard_loss:0.9365079365079365, hammingloss: 0.0125, Precision: 0.9793, Recall: 0.9555, F1: 0.9672\n",
      "Epoch [569/3000], Train Loss: 0.0451\n",
      "Epoch [570/3000],jacquard_loss:0.921875, hammingloss: 0.0156, Precision: 0.9633, Recall: 0.9555, F1: 0.9593\n",
      "Epoch [570/3000], Train Loss: 0.0449\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [571/3000],jacquard_loss:0.9176470588235294, hammingloss: 0.0164, Precision: 0.9669, Recall: 0.9474, F1: 0.9571\n",
      "Epoch [571/3000], Train Loss: 0.0465\n",
      "Epoch [572/3000],jacquard_loss:0.932806324110672, hammingloss: 0.0133, Precision: 0.9752, Recall: 0.9555, F1: 0.9652\n",
      "Epoch [572/3000], Train Loss: 0.0457\n",
      "Epoch [573/3000],jacquard_loss:0.9254901960784314, hammingloss: 0.0148, Precision: 0.9672, Recall: 0.9555, F1: 0.9613\n",
      "Epoch [573/3000], Train Loss: 0.0448\n",
      "Epoch [574/3000],jacquard_loss:0.9027237354085603, hammingloss: 0.0195, Precision: 0.9587, Recall: 0.9393, F1: 0.9489\n",
      "Epoch [574/3000], Train Loss: 0.0503\n",
      "Epoch [575/3000],jacquard_loss:0.9094488188976378, hammingloss: 0.0180, Precision: 0.9706, Recall: 0.9352, F1: 0.9526\n",
      "Epoch [575/3000], Train Loss: 0.0483\n",
      "Epoch [576/3000],jacquard_loss:0.9147286821705426, hammingloss: 0.0172, Precision: 0.9555, Recall: 0.9555, F1: 0.9555\n",
      "Epoch [576/3000], Train Loss: 0.0474\n",
      "Epoch [577/3000],jacquard_loss:0.90625, hammingloss: 0.0187, Precision: 0.9627, Recall: 0.9393, F1: 0.9508\n",
      "Epoch [577/3000], Train Loss: 0.0488\n",
      "Epoch [578/3000],jacquard_loss:0.89272030651341, hammingloss: 0.0219, Precision: 0.9433, Recall: 0.9433, F1: 0.9433\n",
      "Epoch [578/3000], Train Loss: 0.0507\n",
      "Epoch [579/3000],jacquard_loss:0.914396887159533, hammingloss: 0.0172, Precision: 0.9592, Recall: 0.9514, F1: 0.9553\n",
      "Epoch [579/3000], Train Loss: 0.0478\n",
      "Epoch [580/3000],jacquard_loss:0.9027237354085603, hammingloss: 0.0195, Precision: 0.9587, Recall: 0.9393, F1: 0.9489\n",
      "Epoch [580/3000], Train Loss: 0.0466\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [581/3000],jacquard_loss:0.91796875, hammingloss: 0.0164, Precision: 0.9631, Recall: 0.9514, F1: 0.9572\n",
      "Epoch [581/3000], Train Loss: 0.0431\n",
      "Epoch [582/3000],jacquard_loss:0.9019607843137255, hammingloss: 0.0195, Precision: 0.9664, Recall: 0.9312, F1: 0.9485\n",
      "Epoch [582/3000], Train Loss: 0.0463\n",
      "Epoch [583/3000],jacquard_loss:0.9407114624505929, hammingloss: 0.0117, Precision: 0.9754, Recall: 0.9636, F1: 0.9695\n",
      "Epoch [583/3000], Train Loss: 0.0460\n",
      "Epoch [584/3000],jacquard_loss:0.921875, hammingloss: 0.0156, Precision: 0.9633, Recall: 0.9555, F1: 0.9593\n",
      "Epoch [584/3000], Train Loss: 0.0444\n",
      "Epoch [585/3000],jacquard_loss:0.9322709163346613, hammingloss: 0.0133, Precision: 0.9832, Recall: 0.9474, F1: 0.9649\n",
      "Epoch [585/3000], Train Loss: 0.0427\n",
      "Epoch [586/3000],jacquard_loss:0.9182879377431906, hammingloss: 0.0164, Precision: 0.9593, Recall: 0.9555, F1: 0.9574\n",
      "Epoch [586/3000], Train Loss: 0.0432\n",
      "Epoch [587/3000],jacquard_loss:0.9173228346456693, hammingloss: 0.0164, Precision: 0.9708, Recall: 0.9433, F1: 0.9569\n",
      "Epoch [587/3000], Train Loss: 0.0438\n",
      "Epoch [588/3000],jacquard_loss:0.9221789883268483, hammingloss: 0.0156, Precision: 0.9595, Recall: 0.9595, F1: 0.9595\n",
      "Epoch [588/3000], Train Loss: 0.0426\n",
      "Epoch [589/3000],jacquard_loss:0.9442231075697212, hammingloss: 0.0109, Precision: 0.9834, Recall: 0.9595, F1: 0.9713\n",
      "Epoch [589/3000], Train Loss: 0.0418\n",
      "Epoch [590/3000],jacquard_loss:0.9031007751937985, hammingloss: 0.0195, Precision: 0.9549, Recall: 0.9433, F1: 0.9491\n",
      "Epoch [590/3000], Train Loss: 0.0474\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [591/3000],jacquard_loss:0.9137254901960784, hammingloss: 0.0172, Precision: 0.9668, Recall: 0.9433, F1: 0.9549\n",
      "Epoch [591/3000], Train Loss: 0.0467\n",
      "Epoch [592/3000],jacquard_loss:0.9058823529411765, hammingloss: 0.0187, Precision: 0.9665, Recall: 0.9352, F1: 0.9506\n",
      "Epoch [592/3000], Train Loss: 0.0480\n",
      "Epoch [593/3000],jacquard_loss:0.9404761904761905, hammingloss: 0.0117, Precision: 0.9793, Recall: 0.9595, F1: 0.9693\n",
      "Epoch [593/3000], Train Loss: 0.0417\n",
      "Epoch [594/3000],jacquard_loss:0.9034749034749034, hammingloss: 0.0195, Precision: 0.9512, Recall: 0.9474, F1: 0.9493\n",
      "Epoch [594/3000], Train Loss: 0.0467\n",
      "Epoch [595/3000],jacquard_loss:0.9254901960784314, hammingloss: 0.0148, Precision: 0.9672, Recall: 0.9555, F1: 0.9613\n",
      "Epoch [595/3000], Train Loss: 0.0461\n",
      "Epoch [596/3000],jacquard_loss:0.9, hammingloss: 0.0203, Precision: 0.9474, Recall: 0.9474, F1: 0.9474\n",
      "Epoch [596/3000], Train Loss: 0.0519\n",
      "Epoch [597/3000],jacquard_loss:0.888030888030888, hammingloss: 0.0227, Precision: 0.9504, Recall: 0.9312, F1: 0.9407\n",
      "Epoch [597/3000], Train Loss: 0.0531\n",
      "Epoch [598/3000],jacquard_loss:0.8778625954198473, hammingloss: 0.0250, Precision: 0.9388, Recall: 0.9312, F1: 0.9350\n",
      "Epoch [598/3000], Train Loss: 0.0556\n",
      "Epoch [599/3000],jacquard_loss:0.8735632183908046, hammingloss: 0.0258, Precision: 0.9421, Recall: 0.9231, F1: 0.9325\n",
      "Epoch [599/3000], Train Loss: 0.0544\n",
      "Epoch [600/3000],jacquard_loss:0.8560606060606061, hammingloss: 0.0297, Precision: 0.9300, Recall: 0.9150, F1: 0.9224\n",
      "Epoch [600/3000], Train Loss: 0.0638\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [601/3000],jacquard_loss:0.8458646616541353, hammingloss: 0.0320, Precision: 0.9221, Recall: 0.9109, F1: 0.9165\n",
      "Epoch [601/3000], Train Loss: 0.0761\n",
      "Epoch [602/3000],jacquard_loss:0.7906137184115524, hammingloss: 0.0453, Precision: 0.8795, Recall: 0.8866, F1: 0.8831\n",
      "Epoch [602/3000], Train Loss: 0.0898\n",
      "Epoch [603/3000],jacquard_loss:0.8065693430656934, hammingloss: 0.0414, Precision: 0.8911, Recall: 0.8947, F1: 0.8929\n",
      "Epoch [603/3000], Train Loss: 0.0872\n",
      "Epoch [604/3000],jacquard_loss:0.8007246376811594, hammingloss: 0.0430, Precision: 0.8840, Recall: 0.8947, F1: 0.8893\n",
      "Epoch [604/3000], Train Loss: 0.0862\n",
      "Epoch [605/3000],jacquard_loss:0.8154981549815498, hammingloss: 0.0391, Precision: 0.9020, Recall: 0.8947, F1: 0.8984\n",
      "Epoch [605/3000], Train Loss: 0.0875\n",
      "Epoch [606/3000],jacquard_loss:0.7822878228782287, hammingloss: 0.0461, Precision: 0.8983, Recall: 0.8583, F1: 0.8778\n",
      "Epoch [606/3000], Train Loss: 0.0947\n",
      "Epoch [607/3000],jacquard_loss:0.8395522388059702, hammingloss: 0.0336, Precision: 0.9146, Recall: 0.9109, F1: 0.9128\n",
      "Epoch [607/3000], Train Loss: 0.0681\n",
      "Epoch [608/3000],jacquard_loss:0.8798449612403101, hammingloss: 0.0242, Precision: 0.9538, Recall: 0.9190, F1: 0.9361\n",
      "Epoch [608/3000], Train Loss: 0.0591\n",
      "Epoch [609/3000],jacquard_loss:0.8745247148288974, hammingloss: 0.0258, Precision: 0.9350, Recall: 0.9312, F1: 0.9331\n",
      "Epoch [609/3000], Train Loss: 0.0560\n",
      "Epoch [610/3000],jacquard_loss:0.872093023255814, hammingloss: 0.0258, Precision: 0.9534, Recall: 0.9109, F1: 0.9317\n",
      "Epoch [610/3000], Train Loss: 0.0522\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [611/3000],jacquard_loss:0.90625, hammingloss: 0.0187, Precision: 0.9627, Recall: 0.9393, F1: 0.9508\n",
      "Epoch [611/3000], Train Loss: 0.0485\n",
      "Epoch [612/3000],jacquard_loss:0.9076923076923077, hammingloss: 0.0187, Precision: 0.9478, Recall: 0.9555, F1: 0.9516\n",
      "Epoch [612/3000], Train Loss: 0.0483\n",
      "Epoch [613/3000],jacquard_loss:0.8984375, hammingloss: 0.0203, Precision: 0.9623, Recall: 0.9312, F1: 0.9465\n",
      "Epoch [613/3000], Train Loss: 0.0465\n",
      "Epoch [614/3000],jacquard_loss:0.8484848484848485, hammingloss: 0.0312, Precision: 0.9295, Recall: 0.9069, F1: 0.9180\n",
      "Epoch [614/3000], Train Loss: 0.0642\n",
      "Epoch [615/3000],jacquard_loss:0.7992565055762082, hammingloss: 0.0422, Precision: 0.9072, Recall: 0.8704, F1: 0.8884\n",
      "Epoch [615/3000], Train Loss: 0.0848\n",
      "Epoch [616/3000],jacquard_loss:0.868421052631579, hammingloss: 0.0273, Precision: 0.9240, Recall: 0.9352, F1: 0.9296\n",
      "Epoch [616/3000], Train Loss: 0.0648\n",
      "Epoch [617/3000],jacquard_loss:0.8784313725490196, hammingloss: 0.0242, Precision: 0.9655, Recall: 0.9069, F1: 0.9353\n",
      "Epoch [617/3000], Train Loss: 0.0511\n",
      "Epoch [618/3000],jacquard_loss:0.8593155893536122, hammingloss: 0.0289, Precision: 0.9339, Recall: 0.9150, F1: 0.9243\n",
      "Epoch [618/3000], Train Loss: 0.0659\n",
      "Epoch [619/3000],jacquard_loss:0.8296296296296296, hammingloss: 0.0359, Precision: 0.9069, Recall: 0.9069, F1: 0.9069\n",
      "Epoch [619/3000], Train Loss: 0.0777\n",
      "Epoch [620/3000],jacquard_loss:0.7653429602888087, hammingloss: 0.0508, Precision: 0.8760, Recall: 0.8583, F1: 0.8671\n",
      "Epoch [620/3000], Train Loss: 0.1198\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [621/3000],jacquard_loss:0.7491039426523297, hammingloss: 0.0547, Precision: 0.8672, Recall: 0.8462, F1: 0.8566\n",
      "Epoch [621/3000], Train Loss: 0.1246\n",
      "Epoch [622/3000],jacquard_loss:0.7971014492753623, hammingloss: 0.0437, Precision: 0.8835, Recall: 0.8907, F1: 0.8871\n",
      "Epoch [622/3000], Train Loss: 0.1017\n",
      "Epoch [623/3000],jacquard_loss:0.7428571428571429, hammingloss: 0.0563, Precision: 0.8631, Recall: 0.8421, F1: 0.8525\n",
      "Epoch [623/3000], Train Loss: 0.1284\n",
      "Epoch [624/3000],jacquard_loss:0.7021276595744681, hammingloss: 0.0656, Precision: 0.8498, Recall: 0.8016, F1: 0.8250\n",
      "Epoch [624/3000], Train Loss: 0.1613\n",
      "Epoch [625/3000],jacquard_loss:0.7019867549668874, hammingloss: 0.0703, Precision: 0.7940, Recall: 0.8583, F1: 0.8249\n",
      "Epoch [625/3000], Train Loss: 0.1648\n",
      "Epoch [626/3000],jacquard_loss:0.68, hammingloss: 0.0688, Precision: 0.8698, Recall: 0.7571, F1: 0.8095\n",
      "Epoch [626/3000], Train Loss: 0.2058\n",
      "Epoch [627/3000],jacquard_loss:0.74, hammingloss: 0.0609, Precision: 0.8073, Recall: 0.8988, F1: 0.8506\n",
      "Epoch [627/3000], Train Loss: 0.1409\n",
      "Epoch [628/3000],jacquard_loss:0.7662835249042146, hammingloss: 0.0477, Precision: 0.9346, Recall: 0.8097, F1: 0.8677\n",
      "Epoch [628/3000], Train Loss: 0.1159\n",
      "Epoch [629/3000],jacquard_loss:0.8392857142857143, hammingloss: 0.0352, Precision: 0.8769, Recall: 0.9514, F1: 0.9126\n",
      "Epoch [629/3000], Train Loss: 0.0716\n",
      "Epoch [630/3000],jacquard_loss:0.875, hammingloss: 0.0250, Precision: 0.9614, Recall: 0.9069, F1: 0.9333\n",
      "Epoch [630/3000], Train Loss: 0.0524\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [631/3000],jacquard_loss:0.9058823529411765, hammingloss: 0.0187, Precision: 0.9665, Recall: 0.9352, F1: 0.9506\n",
      "Epoch [631/3000], Train Loss: 0.0505\n",
      "Epoch [632/3000],jacquard_loss:0.8977272727272727, hammingloss: 0.0211, Precision: 0.9331, Recall: 0.9595, F1: 0.9461\n",
      "Epoch [632/3000], Train Loss: 0.0459\n",
      "Epoch [633/3000],jacquard_loss:0.8932806324110671, hammingloss: 0.0211, Precision: 0.9741, Recall: 0.9150, F1: 0.9436\n",
      "Epoch [633/3000], Train Loss: 0.0455\n",
      "Epoch [634/3000],jacquard_loss:0.91796875, hammingloss: 0.0164, Precision: 0.9631, Recall: 0.9514, F1: 0.9572\n",
      "Epoch [634/3000], Train Loss: 0.0399\n",
      "Epoch [635/3000],jacquard_loss:0.924901185770751, hammingloss: 0.0148, Precision: 0.9750, Recall: 0.9474, F1: 0.9610\n",
      "Epoch [635/3000], Train Loss: 0.0404\n",
      "Epoch [636/3000],jacquard_loss:0.9291338582677166, hammingloss: 0.0141, Precision: 0.9712, Recall: 0.9555, F1: 0.9633\n",
      "Epoch [636/3000], Train Loss: 0.0370\n",
      "Epoch [637/3000],jacquard_loss:0.9322709163346613, hammingloss: 0.0133, Precision: 0.9832, Recall: 0.9474, F1: 0.9649\n",
      "Epoch [637/3000], Train Loss: 0.0367\n",
      "Epoch [638/3000],jacquard_loss:0.9523809523809523, hammingloss: 0.0094, Precision: 0.9796, Recall: 0.9717, F1: 0.9756\n",
      "Epoch [638/3000], Train Loss: 0.0347\n",
      "Epoch [639/3000],jacquard_loss:0.9561752988047809, hammingloss: 0.0086, Precision: 0.9836, Recall: 0.9717, F1: 0.9776\n",
      "Epoch [639/3000], Train Loss: 0.0344\n",
      "Epoch [640/3000],jacquard_loss:0.96, hammingloss: 0.0078, Precision: 0.9877, Recall: 0.9717, F1: 0.9796\n",
      "Epoch [640/3000], Train Loss: 0.0333\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [641/3000],jacquard_loss:0.9601593625498008, hammingloss: 0.0078, Precision: 0.9837, Recall: 0.9757, F1: 0.9797\n",
      "Epoch [641/3000], Train Loss: 0.0330\n",
      "Epoch [642/3000],jacquard_loss:0.964, hammingloss: 0.0070, Precision: 0.9877, Recall: 0.9757, F1: 0.9817\n",
      "Epoch [642/3000], Train Loss: 0.0324\n",
      "Epoch [643/3000],jacquard_loss:0.964, hammingloss: 0.0070, Precision: 0.9877, Recall: 0.9757, F1: 0.9817\n",
      "Epoch [643/3000], Train Loss: 0.0320\n",
      "Epoch [644/3000],jacquard_loss:0.964, hammingloss: 0.0070, Precision: 0.9877, Recall: 0.9757, F1: 0.9817\n",
      "Epoch [644/3000], Train Loss: 0.0316\n",
      "Epoch [645/3000],jacquard_loss:0.964, hammingloss: 0.0070, Precision: 0.9877, Recall: 0.9757, F1: 0.9817\n",
      "Epoch [645/3000], Train Loss: 0.0313\n",
      "Epoch [646/3000],jacquard_loss:0.964, hammingloss: 0.0070, Precision: 0.9877, Recall: 0.9757, F1: 0.9817\n",
      "Epoch [646/3000], Train Loss: 0.0308\n",
      "Epoch [647/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [647/3000], Train Loss: 0.0305\n",
      "Epoch [648/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [648/3000], Train Loss: 0.0301\n",
      "Epoch [649/3000],jacquard_loss:0.9678714859437751, hammingloss: 0.0063, Precision: 0.9918, Recall: 0.9757, F1: 0.9837\n",
      "Epoch [649/3000], Train Loss: 0.0299\n",
      "Epoch [650/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [650/3000], Train Loss: 0.0295\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [651/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [651/3000], Train Loss: 0.0293\n",
      "Epoch [652/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [652/3000], Train Loss: 0.0289\n",
      "Epoch [653/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [653/3000], Train Loss: 0.0287\n",
      "Epoch [654/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [654/3000], Train Loss: 0.0284\n",
      "Epoch [655/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [655/3000], Train Loss: 0.0282\n",
      "Epoch [656/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [656/3000], Train Loss: 0.0278\n",
      "Epoch [657/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [657/3000], Train Loss: 0.0277\n",
      "Epoch [658/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [658/3000], Train Loss: 0.0272\n",
      "Epoch [659/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [659/3000], Train Loss: 0.0273\n",
      "Epoch [660/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [660/3000], Train Loss: 0.0267\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [661/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [661/3000], Train Loss: 0.0267\n",
      "Epoch [662/3000],jacquard_loss:0.9718875502008032, hammingloss: 0.0055, Precision: 0.9918, Recall: 0.9798, F1: 0.9857\n",
      "Epoch [662/3000], Train Loss: 0.0263\n",
      "Epoch [663/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [663/3000], Train Loss: 0.0262\n",
      "Epoch [664/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [664/3000], Train Loss: 0.0258\n",
      "Epoch [665/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [665/3000], Train Loss: 0.0258\n",
      "Epoch [666/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [666/3000], Train Loss: 0.0253\n",
      "Epoch [667/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [667/3000], Train Loss: 0.0254\n",
      "Epoch [668/3000],jacquard_loss:0.9641434262948207, hammingloss: 0.0070, Precision: 0.9837, Recall: 0.9798, F1: 0.9817\n",
      "Epoch [668/3000], Train Loss: 0.0255\n",
      "Epoch [669/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [669/3000], Train Loss: 0.0251\n",
      "Epoch [670/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [670/3000], Train Loss: 0.0247\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [671/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [671/3000], Train Loss: 0.0249\n",
      "Epoch [672/3000],jacquard_loss:0.9641434262948207, hammingloss: 0.0070, Precision: 0.9837, Recall: 0.9798, F1: 0.9817\n",
      "Epoch [672/3000], Train Loss: 0.0246\n",
      "Epoch [673/3000],jacquard_loss:0.9641434262948207, hammingloss: 0.0070, Precision: 0.9837, Recall: 0.9798, F1: 0.9817\n",
      "Epoch [673/3000], Train Loss: 0.0249\n",
      "Epoch [674/3000],jacquard_loss:0.9681274900398407, hammingloss: 0.0063, Precision: 0.9838, Recall: 0.9838, F1: 0.9838\n",
      "Epoch [674/3000], Train Loss: 0.0243\n",
      "Epoch [675/3000],jacquard_loss:0.968, hammingloss: 0.0063, Precision: 0.9878, Recall: 0.9798, F1: 0.9837\n",
      "Epoch [675/3000], Train Loss: 0.0248\n",
      "Epoch [676/3000],jacquard_loss:0.9681274900398407, hammingloss: 0.0063, Precision: 0.9838, Recall: 0.9838, F1: 0.9838\n",
      "Epoch [676/3000], Train Loss: 0.0240\n",
      "Epoch [677/3000],jacquard_loss:0.9641434262948207, hammingloss: 0.0070, Precision: 0.9837, Recall: 0.9798, F1: 0.9817\n",
      "Epoch [677/3000], Train Loss: 0.0250\n",
      "Epoch [678/3000],jacquard_loss:0.9681274900398407, hammingloss: 0.0063, Precision: 0.9838, Recall: 0.9838, F1: 0.9838\n",
      "Epoch [678/3000], Train Loss: 0.0242\n",
      "Epoch [679/3000],jacquard_loss:0.9641434262948207, hammingloss: 0.0070, Precision: 0.9837, Recall: 0.9798, F1: 0.9817\n",
      "Epoch [679/3000], Train Loss: 0.0253\n",
      "Epoch [680/3000],jacquard_loss:0.9681274900398407, hammingloss: 0.0063, Precision: 0.9838, Recall: 0.9838, F1: 0.9838\n",
      "Epoch [680/3000], Train Loss: 0.0246\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [681/3000],jacquard_loss:0.9484126984126984, hammingloss: 0.0102, Precision: 0.9795, Recall: 0.9676, F1: 0.9735\n",
      "Epoch [681/3000], Train Loss: 0.0260\n",
      "Epoch [682/3000],jacquard_loss:0.9642857142857143, hammingloss: 0.0070, Precision: 0.9798, Recall: 0.9838, F1: 0.9818\n",
      "Epoch [682/3000], Train Loss: 0.0254\n",
      "Epoch [683/3000],jacquard_loss:0.9482071713147411, hammingloss: 0.0102, Precision: 0.9835, Recall: 0.9636, F1: 0.9734\n",
      "Epoch [683/3000], Train Loss: 0.0269\n",
      "Epoch [684/3000],jacquard_loss:0.9568627450980393, hammingloss: 0.0086, Precision: 0.9683, Recall: 0.9879, F1: 0.9780\n",
      "Epoch [684/3000], Train Loss: 0.0266\n",
      "Epoch [685/3000],jacquard_loss:0.9444444444444444, hammingloss: 0.0109, Precision: 0.9794, Recall: 0.9636, F1: 0.9714\n",
      "Epoch [685/3000], Train Loss: 0.0279\n",
      "Epoch [686/3000],jacquard_loss:0.953125, hammingloss: 0.0094, Precision: 0.9644, Recall: 0.9879, F1: 0.9760\n",
      "Epoch [686/3000], Train Loss: 0.0278\n",
      "Epoch [687/3000],jacquard_loss:0.9444444444444444, hammingloss: 0.0109, Precision: 0.9794, Recall: 0.9636, F1: 0.9714\n",
      "Epoch [687/3000], Train Loss: 0.0292\n",
      "Epoch [688/3000],jacquard_loss:0.94921875, hammingloss: 0.0102, Precision: 0.9643, Recall: 0.9838, F1: 0.9739\n",
      "Epoch [688/3000], Train Loss: 0.0297\n",
      "Epoch [689/3000],jacquard_loss:0.9442231075697212, hammingloss: 0.0109, Precision: 0.9834, Recall: 0.9595, F1: 0.9713\n",
      "Epoch [689/3000], Train Loss: 0.0309\n",
      "Epoch [690/3000],jacquard_loss:0.9418604651162791, hammingloss: 0.0117, Precision: 0.9567, Recall: 0.9838, F1: 0.9701\n",
      "Epoch [690/3000], Train Loss: 0.0334\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [691/3000],jacquard_loss:0.9442231075697212, hammingloss: 0.0109, Precision: 0.9834, Recall: 0.9595, F1: 0.9713\n",
      "Epoch [691/3000], Train Loss: 0.0327\n",
      "Epoch [692/3000],jacquard_loss:0.9307692307692308, hammingloss: 0.0141, Precision: 0.9490, Recall: 0.9798, F1: 0.9641\n",
      "Epoch [692/3000], Train Loss: 0.0370\n",
      "Epoch [693/3000],jacquard_loss:0.9402390438247012, hammingloss: 0.0117, Precision: 0.9833, Recall: 0.9555, F1: 0.9692\n",
      "Epoch [693/3000], Train Loss: 0.0342\n",
      "Epoch [694/3000],jacquard_loss:0.9236641221374046, hammingloss: 0.0156, Precision: 0.9416, Recall: 0.9798, F1: 0.9603\n",
      "Epoch [694/3000], Train Loss: 0.0367\n",
      "Epoch [695/3000],jacquard_loss:0.9322709163346613, hammingloss: 0.0133, Precision: 0.9832, Recall: 0.9474, F1: 0.9649\n",
      "Epoch [695/3000], Train Loss: 0.0363\n",
      "Epoch [696/3000],jacquard_loss:0.9496124031007752, hammingloss: 0.0102, Precision: 0.9570, Recall: 0.9919, F1: 0.9742\n",
      "Epoch [696/3000], Train Loss: 0.0350\n",
      "Epoch [697/3000],jacquard_loss:0.9246031746031746, hammingloss: 0.0148, Precision: 0.9790, Recall: 0.9433, F1: 0.9608\n",
      "Epoch [697/3000], Train Loss: 0.0375\n",
      "Epoch [698/3000],jacquard_loss:0.9488188976377953, hammingloss: 0.0102, Precision: 0.9718, Recall: 0.9757, F1: 0.9737\n",
      "Epoch [698/3000], Train Loss: 0.0291\n",
      "Epoch [699/3000],jacquard_loss:0.9565217391304348, hammingloss: 0.0086, Precision: 0.9758, Recall: 0.9798, F1: 0.9778\n",
      "Epoch [699/3000], Train Loss: 0.0289\n",
      "Epoch [700/3000],jacquard_loss:0.972, hammingloss: 0.0055, Precision: 0.9878, Recall: 0.9838, F1: 0.9858\n",
      "Epoch [700/3000], Train Loss: 0.0230\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [701/3000],jacquard_loss:0.972, hammingloss: 0.0055, Precision: 0.9878, Recall: 0.9838, F1: 0.9858\n",
      "Epoch [701/3000], Train Loss: 0.0203\n",
      "Epoch [702/3000],jacquard_loss:0.9641434262948207, hammingloss: 0.0070, Precision: 0.9837, Recall: 0.9798, F1: 0.9817\n",
      "Epoch [702/3000], Train Loss: 0.0212\n",
      "Epoch [703/3000],jacquard_loss:0.9759036144578314, hammingloss: 0.0047, Precision: 0.9918, Recall: 0.9838, F1: 0.9878\n",
      "Epoch [703/3000], Train Loss: 0.0203\n",
      "Epoch [704/3000],jacquard_loss:0.972, hammingloss: 0.0055, Precision: 0.9878, Recall: 0.9838, F1: 0.9858\n",
      "Epoch [704/3000], Train Loss: 0.0198\n",
      "Epoch [705/3000],jacquard_loss:0.9681274900398407, hammingloss: 0.0063, Precision: 0.9838, Recall: 0.9838, F1: 0.9838\n",
      "Epoch [705/3000], Train Loss: 0.0194\n",
      "Epoch [706/3000],jacquard_loss:0.972, hammingloss: 0.0055, Precision: 0.9878, Recall: 0.9838, F1: 0.9858\n",
      "Epoch [706/3000], Train Loss: 0.0191\n",
      "Epoch [707/3000],jacquard_loss:0.972, hammingloss: 0.0055, Precision: 0.9878, Recall: 0.9838, F1: 0.9858\n",
      "Epoch [707/3000], Train Loss: 0.0190\n",
      "Epoch [708/3000],jacquard_loss:0.9799196787148594, hammingloss: 0.0039, Precision: 0.9919, Recall: 0.9879, F1: 0.9899\n",
      "Epoch [708/3000], Train Loss: 0.0189\n",
      "Epoch [709/3000],jacquard_loss:0.976, hammingloss: 0.0047, Precision: 0.9879, Recall: 0.9879, F1: 0.9879\n",
      "Epoch [709/3000], Train Loss: 0.0186\n",
      "Epoch [710/3000],jacquard_loss:0.976, hammingloss: 0.0047, Precision: 0.9879, Recall: 0.9879, F1: 0.9879\n",
      "Epoch [710/3000], Train Loss: 0.0186\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [711/3000],jacquard_loss:0.972, hammingloss: 0.0055, Precision: 0.9878, Recall: 0.9838, F1: 0.9858\n",
      "Epoch [711/3000], Train Loss: 0.0183\n",
      "Epoch [712/3000],jacquard_loss:0.9839357429718876, hammingloss: 0.0031, Precision: 0.9919, Recall: 0.9919, F1: 0.9919\n",
      "Epoch [712/3000], Train Loss: 0.0182\n",
      "Epoch [713/3000],jacquard_loss:0.976, hammingloss: 0.0047, Precision: 0.9879, Recall: 0.9879, F1: 0.9879\n",
      "Epoch [713/3000], Train Loss: 0.0181\n",
      "Epoch [714/3000],jacquard_loss:0.976, hammingloss: 0.0047, Precision: 0.9879, Recall: 0.9879, F1: 0.9879\n",
      "Epoch [714/3000], Train Loss: 0.0180\n",
      "Epoch [715/3000],jacquard_loss:0.9799196787148594, hammingloss: 0.0039, Precision: 0.9919, Recall: 0.9879, F1: 0.9899\n",
      "Epoch [715/3000], Train Loss: 0.0178\n",
      "Epoch [716/3000],jacquard_loss:0.9839357429718876, hammingloss: 0.0031, Precision: 0.9919, Recall: 0.9919, F1: 0.9919\n",
      "Epoch [716/3000], Train Loss: 0.0178\n",
      "Epoch [717/3000],jacquard_loss:0.9799196787148594, hammingloss: 0.0039, Precision: 0.9919, Recall: 0.9879, F1: 0.9899\n",
      "Epoch [717/3000], Train Loss: 0.0175\n",
      "Epoch [718/3000],jacquard_loss:0.98, hammingloss: 0.0039, Precision: 0.9879, Recall: 0.9919, F1: 0.9899\n",
      "Epoch [718/3000], Train Loss: 0.0175\n",
      "Epoch [719/3000],jacquard_loss:0.9838709677419355, hammingloss: 0.0031, Precision: 0.9959, Recall: 0.9879, F1: 0.9919\n",
      "Epoch [719/3000], Train Loss: 0.0174\n",
      "Epoch [720/3000],jacquard_loss:0.976, hammingloss: 0.0047, Precision: 0.9879, Recall: 0.9879, F1: 0.9879\n",
      "Epoch [720/3000], Train Loss: 0.0173\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [721/3000],jacquard_loss:0.9838709677419355, hammingloss: 0.0031, Precision: 0.9959, Recall: 0.9879, F1: 0.9919\n",
      "Epoch [721/3000], Train Loss: 0.0170\n",
      "Epoch [722/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [722/3000], Train Loss: 0.0169\n",
      "Epoch [723/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [723/3000], Train Loss: 0.0168\n",
      "Epoch [724/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [724/3000], Train Loss: 0.0169\n",
      "Epoch [725/3000],jacquard_loss:0.9838709677419355, hammingloss: 0.0031, Precision: 0.9959, Recall: 0.9879, F1: 0.9919\n",
      "Epoch [725/3000], Train Loss: 0.0167\n",
      "Epoch [726/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [726/3000], Train Loss: 0.0167\n",
      "Epoch [727/3000],jacquard_loss:0.9798387096774194, hammingloss: 0.0039, Precision: 0.9959, Recall: 0.9838, F1: 0.9898\n",
      "Epoch [727/3000], Train Loss: 0.0165\n",
      "Epoch [728/3000],jacquard_loss:0.9839357429718876, hammingloss: 0.0031, Precision: 0.9919, Recall: 0.9919, F1: 0.9919\n",
      "Epoch [728/3000], Train Loss: 0.0162\n",
      "Epoch [729/3000],jacquard_loss:0.9838709677419355, hammingloss: 0.0031, Precision: 0.9959, Recall: 0.9879, F1: 0.9919\n",
      "Epoch [729/3000], Train Loss: 0.0162\n",
      "Epoch [730/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [730/3000], Train Loss: 0.0162\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [731/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [731/3000], Train Loss: 0.0160\n",
      "Epoch [732/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [732/3000], Train Loss: 0.0160\n",
      "Epoch [733/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [733/3000], Train Loss: 0.0158\n",
      "Epoch [734/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [734/3000], Train Loss: 0.0157\n",
      "Epoch [735/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [735/3000], Train Loss: 0.0156\n",
      "Epoch [736/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [736/3000], Train Loss: 0.0157\n",
      "Epoch [737/3000],jacquard_loss:0.9838709677419355, hammingloss: 0.0031, Precision: 0.9959, Recall: 0.9879, F1: 0.9919\n",
      "Epoch [737/3000], Train Loss: 0.0155\n",
      "Epoch [738/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [738/3000], Train Loss: 0.0154\n",
      "Epoch [739/3000],jacquard_loss:0.9838709677419355, hammingloss: 0.0031, Precision: 0.9959, Recall: 0.9879, F1: 0.9919\n",
      "Epoch [739/3000], Train Loss: 0.0152\n",
      "Epoch [740/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [740/3000], Train Loss: 0.0151\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [741/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [741/3000], Train Loss: 0.0150\n",
      "Epoch [742/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [742/3000], Train Loss: 0.0151\n",
      "Epoch [743/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [743/3000], Train Loss: 0.0149\n",
      "Epoch [744/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [744/3000], Train Loss: 0.0149\n",
      "Epoch [745/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [745/3000], Train Loss: 0.0147\n",
      "Epoch [746/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [746/3000], Train Loss: 0.0146\n",
      "Epoch [747/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [747/3000], Train Loss: 0.0145\n",
      "Epoch [748/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [748/3000], Train Loss: 0.0144\n",
      "Epoch [749/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [749/3000], Train Loss: 0.0144\n",
      "Epoch [750/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [750/3000], Train Loss: 0.0143\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [751/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [751/3000], Train Loss: 0.0142\n",
      "Epoch [752/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [752/3000], Train Loss: 0.0142\n",
      "Epoch [753/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [753/3000], Train Loss: 0.0140\n",
      "Epoch [754/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [754/3000], Train Loss: 0.0140\n",
      "Epoch [755/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [755/3000], Train Loss: 0.0139\n",
      "Epoch [756/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [756/3000], Train Loss: 0.0138\n",
      "Epoch [757/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [757/3000], Train Loss: 0.0136\n",
      "Epoch [758/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [758/3000], Train Loss: 0.0137\n",
      "Epoch [759/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [759/3000], Train Loss: 0.0136\n",
      "Epoch [760/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [760/3000], Train Loss: 0.0135\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [761/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [761/3000], Train Loss: 0.0134\n",
      "Epoch [762/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [762/3000], Train Loss: 0.0133\n",
      "Epoch [763/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [763/3000], Train Loss: 0.0131\n",
      "Epoch [764/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [764/3000], Train Loss: 0.0132\n",
      "Epoch [765/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [765/3000], Train Loss: 0.0131\n",
      "Epoch [766/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [766/3000], Train Loss: 0.0131\n",
      "Epoch [767/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [767/3000], Train Loss: 0.0130\n",
      "Epoch [768/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [768/3000], Train Loss: 0.0129\n",
      "Epoch [769/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [769/3000], Train Loss: 0.0128\n",
      "Epoch [770/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [770/3000], Train Loss: 0.0127\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [771/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [771/3000], Train Loss: 0.0126\n",
      "Epoch [772/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [772/3000], Train Loss: 0.0126\n",
      "Epoch [773/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [773/3000], Train Loss: 0.0124\n",
      "Epoch [774/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [774/3000], Train Loss: 0.0124\n",
      "Epoch [775/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [775/3000], Train Loss: 0.0124\n",
      "Epoch [776/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [776/3000], Train Loss: 0.0122\n",
      "Epoch [777/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [777/3000], Train Loss: 0.0122\n",
      "Epoch [778/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [778/3000], Train Loss: 0.0122\n",
      "Epoch [779/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [779/3000], Train Loss: 0.0120\n",
      "Epoch [780/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [780/3000], Train Loss: 0.0122\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [781/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [781/3000], Train Loss: 0.0120\n",
      "Epoch [782/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [782/3000], Train Loss: 0.0120\n",
      "Epoch [783/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [783/3000], Train Loss: 0.0119\n",
      "Epoch [784/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [784/3000], Train Loss: 0.0118\n",
      "Epoch [785/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [785/3000], Train Loss: 0.0117\n",
      "Epoch [786/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [786/3000], Train Loss: 0.0116\n",
      "Epoch [787/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [787/3000], Train Loss: 0.0116\n",
      "Epoch [788/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [788/3000], Train Loss: 0.0116\n",
      "Epoch [789/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [789/3000], Train Loss: 0.0115\n",
      "Epoch [790/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [790/3000], Train Loss: 0.0113\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [791/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [791/3000], Train Loss: 0.0115\n",
      "Epoch [792/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [792/3000], Train Loss: 0.0113\n",
      "Epoch [793/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [793/3000], Train Loss: 0.0112\n",
      "Epoch [794/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [794/3000], Train Loss: 0.0112\n",
      "Epoch [795/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [795/3000], Train Loss: 0.0111\n",
      "Epoch [796/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [796/3000], Train Loss: 0.0111\n",
      "Epoch [797/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [797/3000], Train Loss: 0.0110\n",
      "Epoch [798/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [798/3000], Train Loss: 0.0108\n",
      "Epoch [799/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [799/3000], Train Loss: 0.0108\n",
      "Epoch [800/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [800/3000], Train Loss: 0.0108\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [801/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [801/3000], Train Loss: 0.0108\n",
      "Epoch [802/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [802/3000], Train Loss: 0.0107\n",
      "Epoch [803/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [803/3000], Train Loss: 0.0107\n",
      "Epoch [804/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [804/3000], Train Loss: 0.0106\n",
      "Epoch [805/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [805/3000], Train Loss: 0.0105\n",
      "Epoch [806/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [806/3000], Train Loss: 0.0104\n",
      "Epoch [807/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [807/3000], Train Loss: 0.0104\n",
      "Epoch [808/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [808/3000], Train Loss: 0.0104\n",
      "Epoch [809/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [809/3000], Train Loss: 0.0103\n",
      "Epoch [810/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [810/3000], Train Loss: 0.0103\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [811/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [811/3000], Train Loss: 0.0102\n",
      "Epoch [812/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [812/3000], Train Loss: 0.0102\n",
      "Epoch [813/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [813/3000], Train Loss: 0.0100\n",
      "Epoch [814/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [814/3000], Train Loss: 0.0101\n",
      "Epoch [815/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [815/3000], Train Loss: 0.0100\n",
      "Epoch [816/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [816/3000], Train Loss: 0.0100\n",
      "Epoch [817/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [817/3000], Train Loss: 0.0099\n",
      "Epoch [818/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [818/3000], Train Loss: 0.0099\n",
      "Epoch [819/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [819/3000], Train Loss: 0.0098\n",
      "Epoch [820/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [820/3000], Train Loss: 0.0098\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [821/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [821/3000], Train Loss: 0.0097\n",
      "Epoch [822/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [822/3000], Train Loss: 0.0096\n",
      "Epoch [823/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [823/3000], Train Loss: 0.0096\n",
      "Epoch [824/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [824/3000], Train Loss: 0.0096\n",
      "Epoch [825/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [825/3000], Train Loss: 0.0097\n",
      "Epoch [826/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [826/3000], Train Loss: 0.0095\n",
      "Epoch [827/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [827/3000], Train Loss: 0.0093\n",
      "Epoch [828/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [828/3000], Train Loss: 0.0094\n",
      "Epoch [829/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [829/3000], Train Loss: 0.0094\n",
      "Epoch [830/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [830/3000], Train Loss: 0.0093\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [831/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [831/3000], Train Loss: 0.0093\n",
      "Epoch [832/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [832/3000], Train Loss: 0.0092\n",
      "Epoch [833/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [833/3000], Train Loss: 0.0092\n",
      "Epoch [834/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [834/3000], Train Loss: 0.0091\n",
      "Epoch [835/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [835/3000], Train Loss: 0.0092\n",
      "Epoch [836/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [836/3000], Train Loss: 0.0091\n",
      "Epoch [837/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [837/3000], Train Loss: 0.0089\n",
      "Epoch [838/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [838/3000], Train Loss: 0.0089\n",
      "Epoch [839/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [839/3000], Train Loss: 0.0090\n",
      "Epoch [840/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [840/3000], Train Loss: 0.0088\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [841/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [841/3000], Train Loss: 0.0088\n",
      "Epoch [842/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [842/3000], Train Loss: 0.0088\n",
      "Epoch [843/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [843/3000], Train Loss: 0.0087\n",
      "Epoch [844/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [844/3000], Train Loss: 0.0086\n",
      "Epoch [845/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [845/3000], Train Loss: 0.0086\n",
      "Epoch [846/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [846/3000], Train Loss: 0.0086\n",
      "Epoch [847/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [847/3000], Train Loss: 0.0085\n",
      "Epoch [848/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [848/3000], Train Loss: 0.0086\n",
      "Epoch [849/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [849/3000], Train Loss: 0.0085\n",
      "Epoch [850/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [850/3000], Train Loss: 0.0084\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [851/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [851/3000], Train Loss: 0.0083\n",
      "Epoch [852/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [852/3000], Train Loss: 0.0084\n",
      "Epoch [853/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [853/3000], Train Loss: 0.0084\n",
      "Epoch [854/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [854/3000], Train Loss: 0.0082\n",
      "Epoch [855/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [855/3000], Train Loss: 0.0082\n",
      "Epoch [856/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [856/3000], Train Loss: 0.0082\n",
      "Epoch [857/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [857/3000], Train Loss: 0.0081\n",
      "Epoch [858/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [858/3000], Train Loss: 0.0081\n",
      "Epoch [859/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [859/3000], Train Loss: 0.0081\n",
      "Epoch [860/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [860/3000], Train Loss: 0.0080\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [861/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [861/3000], Train Loss: 0.0080\n",
      "Epoch [862/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [862/3000], Train Loss: 0.0081\n",
      "Epoch [863/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [863/3000], Train Loss: 0.0080\n",
      "Epoch [864/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [864/3000], Train Loss: 0.0079\n",
      "Epoch [865/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [865/3000], Train Loss: 0.0078\n",
      "Epoch [866/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [866/3000], Train Loss: 0.0078\n",
      "Epoch [867/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [867/3000], Train Loss: 0.0078\n",
      "Epoch [868/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [868/3000], Train Loss: 0.0078\n",
      "Epoch [869/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [869/3000], Train Loss: 0.0077\n",
      "Epoch [870/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [870/3000], Train Loss: 0.0077\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [871/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [871/3000], Train Loss: 0.0076\n",
      "Epoch [872/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [872/3000], Train Loss: 0.0076\n",
      "Epoch [873/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [873/3000], Train Loss: 0.0076\n",
      "Epoch [874/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [874/3000], Train Loss: 0.0076\n",
      "Epoch [875/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [875/3000], Train Loss: 0.0075\n",
      "Epoch [876/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [876/3000], Train Loss: 0.0075\n",
      "Epoch [877/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [877/3000], Train Loss: 0.0075\n",
      "Epoch [878/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [878/3000], Train Loss: 0.0075\n",
      "Epoch [879/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [879/3000], Train Loss: 0.0074\n",
      "Epoch [880/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [880/3000], Train Loss: 0.0073\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [881/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [881/3000], Train Loss: 0.0073\n",
      "Epoch [882/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [882/3000], Train Loss: 0.0073\n",
      "Epoch [883/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [883/3000], Train Loss: 0.0073\n",
      "Epoch [884/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [884/3000], Train Loss: 0.0072\n",
      "Epoch [885/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [885/3000], Train Loss: 0.0072\n",
      "Epoch [886/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [886/3000], Train Loss: 0.0072\n",
      "Epoch [887/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [887/3000], Train Loss: 0.0071\n",
      "Epoch [888/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [888/3000], Train Loss: 0.0070\n",
      "Epoch [889/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [889/3000], Train Loss: 0.0071\n",
      "Epoch [890/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [890/3000], Train Loss: 0.0070\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [891/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [891/3000], Train Loss: 0.0070\n",
      "Epoch [892/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [892/3000], Train Loss: 0.0070\n",
      "Epoch [893/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [893/3000], Train Loss: 0.0070\n",
      "Epoch [894/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [894/3000], Train Loss: 0.0069\n",
      "Epoch [895/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [895/3000], Train Loss: 0.0069\n",
      "Epoch [896/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [896/3000], Train Loss: 0.0068\n",
      "Epoch [897/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [897/3000], Train Loss: 0.0068\n",
      "Epoch [898/3000],jacquard_loss:0.9879032258064516, hammingloss: 0.0023, Precision: 0.9959, Recall: 0.9919, F1: 0.9939\n",
      "Epoch [898/3000], Train Loss: 0.0068\n",
      "Epoch [899/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [899/3000], Train Loss: 0.0067\n",
      "Epoch [900/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [900/3000], Train Loss: 0.0068\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [901/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [901/3000], Train Loss: 0.0067\n",
      "Epoch [902/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [902/3000], Train Loss: 0.0067\n",
      "Epoch [903/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [903/3000], Train Loss: 0.0066\n",
      "Epoch [904/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [904/3000], Train Loss: 0.0066\n",
      "Epoch [905/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [905/3000], Train Loss: 0.0066\n",
      "Epoch [906/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [906/3000], Train Loss: 0.0065\n",
      "Epoch [907/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [907/3000], Train Loss: 0.0065\n",
      "Epoch [908/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [908/3000], Train Loss: 0.0065\n",
      "Epoch [909/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [909/3000], Train Loss: 0.0065\n",
      "Epoch [910/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [910/3000], Train Loss: 0.0064\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [911/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [911/3000], Train Loss: 0.0064\n",
      "Epoch [912/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [912/3000], Train Loss: 0.0064\n",
      "Epoch [913/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [913/3000], Train Loss: 0.0064\n",
      "Epoch [914/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [914/3000], Train Loss: 0.0064\n",
      "Epoch [915/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [915/3000], Train Loss: 0.0063\n",
      "Epoch [916/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [916/3000], Train Loss: 0.0063\n",
      "Epoch [917/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [917/3000], Train Loss: 0.0063\n",
      "Epoch [918/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [918/3000], Train Loss: 0.0062\n",
      "Epoch [919/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [919/3000], Train Loss: 0.0062\n",
      "Epoch [920/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [920/3000], Train Loss: 0.0062\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [921/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [921/3000], Train Loss: 0.0062\n",
      "Epoch [922/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [922/3000], Train Loss: 0.0061\n",
      "Epoch [923/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [923/3000], Train Loss: 0.0061\n",
      "Epoch [924/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [924/3000], Train Loss: 0.0061\n",
      "Epoch [925/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [925/3000], Train Loss: 0.0061\n",
      "Epoch [926/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [926/3000], Train Loss: 0.0061\n",
      "Epoch [927/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [927/3000], Train Loss: 0.0060\n",
      "Epoch [928/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [928/3000], Train Loss: 0.0060\n",
      "Epoch [929/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [929/3000], Train Loss: 0.0059\n",
      "Epoch [930/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [930/3000], Train Loss: 0.0060\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [931/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [931/3000], Train Loss: 0.0059\n",
      "Epoch [932/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [932/3000], Train Loss: 0.0059\n",
      "Epoch [933/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [933/3000], Train Loss: 0.0060\n",
      "Epoch [934/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [934/3000], Train Loss: 0.0058\n",
      "Epoch [935/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [935/3000], Train Loss: 0.0058\n",
      "Epoch [936/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [936/3000], Train Loss: 0.0058\n",
      "Epoch [937/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [937/3000], Train Loss: 0.0058\n",
      "Epoch [938/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [938/3000], Train Loss: 0.0058\n",
      "Epoch [939/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [939/3000], Train Loss: 0.0057\n",
      "Epoch [940/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [940/3000], Train Loss: 0.0057\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [941/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [941/3000], Train Loss: 0.0058\n",
      "Epoch [942/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [942/3000], Train Loss: 0.0057\n",
      "Epoch [943/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [943/3000], Train Loss: 0.0056\n",
      "Epoch [944/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [944/3000], Train Loss: 0.0056\n",
      "Epoch [945/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [945/3000], Train Loss: 0.0056\n",
      "Epoch [946/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [946/3000], Train Loss: 0.0056\n",
      "Epoch [947/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [947/3000], Train Loss: 0.0055\n",
      "Epoch [948/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [948/3000], Train Loss: 0.0056\n",
      "Epoch [949/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [949/3000], Train Loss: 0.0056\n",
      "Epoch [950/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [950/3000], Train Loss: 0.0055\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [951/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [951/3000], Train Loss: 0.0055\n",
      "Epoch [952/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [952/3000], Train Loss: 0.0055\n",
      "Epoch [953/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [953/3000], Train Loss: 0.0054\n",
      "Epoch [954/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [954/3000], Train Loss: 0.0054\n",
      "Epoch [955/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [955/3000], Train Loss: 0.0054\n",
      "Epoch [956/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [956/3000], Train Loss: 0.0054\n",
      "Epoch [957/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [957/3000], Train Loss: 0.0054\n",
      "Epoch [958/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [958/3000], Train Loss: 0.0053\n",
      "Epoch [959/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [959/3000], Train Loss: 0.0054\n",
      "Epoch [960/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [960/3000], Train Loss: 0.0054\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [961/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [961/3000], Train Loss: 0.0052\n",
      "Epoch [962/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [962/3000], Train Loss: 0.0052\n",
      "Epoch [963/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [963/3000], Train Loss: 0.0052\n",
      "Epoch [964/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [964/3000], Train Loss: 0.0052\n",
      "Epoch [965/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [965/3000], Train Loss: 0.0052\n",
      "Epoch [966/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [966/3000], Train Loss: 0.0052\n",
      "Epoch [967/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [967/3000], Train Loss: 0.0051\n",
      "Epoch [968/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [968/3000], Train Loss: 0.0052\n",
      "Epoch [969/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [969/3000], Train Loss: 0.0052\n",
      "Epoch [970/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [970/3000], Train Loss: 0.0051\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [971/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [971/3000], Train Loss: 0.0051\n",
      "Epoch [972/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [972/3000], Train Loss: 0.0051\n",
      "Epoch [973/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [973/3000], Train Loss: 0.0050\n",
      "Epoch [974/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [974/3000], Train Loss: 0.0050\n",
      "Epoch [975/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [975/3000], Train Loss: 0.0050\n",
      "Epoch [976/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [976/3000], Train Loss: 0.0050\n",
      "Epoch [977/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [977/3000], Train Loss: 0.0050\n",
      "Epoch [978/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [978/3000], Train Loss: 0.0050\n",
      "Epoch [979/3000],jacquard_loss:0.9919354838709677, hammingloss: 0.0016, Precision: 0.9960, Recall: 0.9960, F1: 0.9960\n",
      "Epoch [979/3000], Train Loss: 0.0049\n",
      "Epoch [980/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [980/3000], Train Loss: 0.0049\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [981/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [981/3000], Train Loss: 0.0049\n",
      "Epoch [982/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [982/3000], Train Loss: 0.0049\n",
      "Epoch [983/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [983/3000], Train Loss: 0.0049\n",
      "Epoch [984/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [984/3000], Train Loss: 0.0049\n",
      "Epoch [985/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [985/3000], Train Loss: 0.0048\n",
      "Epoch [986/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [986/3000], Train Loss: 0.0049\n",
      "Epoch [987/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [987/3000], Train Loss: 0.0049\n",
      "Epoch [988/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [988/3000], Train Loss: 0.0048\n",
      "Epoch [989/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [989/3000], Train Loss: 0.0048\n",
      "Epoch [990/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [990/3000], Train Loss: 0.0048\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [991/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [991/3000], Train Loss: 0.0047\n",
      "Epoch [992/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [992/3000], Train Loss: 0.0047\n",
      "Epoch [993/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [993/3000], Train Loss: 0.0047\n",
      "Epoch [994/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [994/3000], Train Loss: 0.0047\n",
      "Epoch [995/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [995/3000], Train Loss: 0.0047\n",
      "Epoch [996/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [996/3000], Train Loss: 0.0047\n",
      "Epoch [997/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [997/3000], Train Loss: 0.0047\n",
      "Epoch [998/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [998/3000], Train Loss: 0.0047\n",
      "Epoch [999/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [999/3000], Train Loss: 0.0046\n",
      "Epoch [1000/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1000/3000], Train Loss: 0.0046\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1001/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1001/3000], Train Loss: 0.0046\n",
      "Epoch [1002/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1002/3000], Train Loss: 0.0046\n",
      "Epoch [1003/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1003/3000], Train Loss: 0.0046\n",
      "Epoch [1004/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1004/3000], Train Loss: 0.0045\n",
      "Epoch [1005/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1005/3000], Train Loss: 0.0045\n",
      "Epoch [1006/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1006/3000], Train Loss: 0.0046\n",
      "Epoch [1007/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1007/3000], Train Loss: 0.0045\n",
      "Epoch [1008/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1008/3000], Train Loss: 0.0045\n",
      "Epoch [1009/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1009/3000], Train Loss: 0.0045\n",
      "Epoch [1010/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1010/3000], Train Loss: 0.0045\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1011/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1011/3000], Train Loss: 0.0045\n",
      "Epoch [1012/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1012/3000], Train Loss: 0.0044\n",
      "Epoch [1013/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1013/3000], Train Loss: 0.0044\n",
      "Epoch [1014/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1014/3000], Train Loss: 0.0044\n",
      "Epoch [1015/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1015/3000], Train Loss: 0.0044\n",
      "Epoch [1016/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1016/3000], Train Loss: 0.0044\n",
      "Epoch [1017/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1017/3000], Train Loss: 0.0044\n",
      "Epoch [1018/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1018/3000], Train Loss: 0.0043\n",
      "Epoch [1019/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1019/3000], Train Loss: 0.0043\n",
      "Epoch [1020/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1020/3000], Train Loss: 0.0043\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1021/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1021/3000], Train Loss: 0.0043\n",
      "Epoch [1022/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1022/3000], Train Loss: 0.0043\n",
      "Epoch [1023/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1023/3000], Train Loss: 0.0043\n",
      "Epoch [1024/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1024/3000], Train Loss: 0.0043\n",
      "Epoch [1025/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1025/3000], Train Loss: 0.0042\n",
      "Epoch [1026/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1026/3000], Train Loss: 0.0042\n",
      "Epoch [1027/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1027/3000], Train Loss: 0.0042\n",
      "Epoch [1028/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1028/3000], Train Loss: 0.0042\n",
      "Epoch [1029/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1029/3000], Train Loss: 0.0042\n",
      "Epoch [1030/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1030/3000], Train Loss: 0.0042\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1031/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1031/3000], Train Loss: 0.0041\n",
      "Epoch [1032/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1032/3000], Train Loss: 0.0042\n",
      "Epoch [1033/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1033/3000], Train Loss: 0.0042\n",
      "Epoch [1034/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1034/3000], Train Loss: 0.0041\n",
      "Epoch [1035/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1035/3000], Train Loss: 0.0041\n",
      "Epoch [1036/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1036/3000], Train Loss: 0.0041\n",
      "Epoch [1037/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1037/3000], Train Loss: 0.0041\n",
      "Epoch [1038/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1038/3000], Train Loss: 0.0041\n",
      "Epoch [1039/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1039/3000], Train Loss: 0.0041\n",
      "Epoch [1040/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1040/3000], Train Loss: 0.0040\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1041/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1041/3000], Train Loss: 0.0040\n",
      "Epoch [1042/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1042/3000], Train Loss: 0.0040\n",
      "Epoch [1043/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1043/3000], Train Loss: 0.0040\n",
      "Epoch [1044/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1044/3000], Train Loss: 0.0040\n",
      "Epoch [1045/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1045/3000], Train Loss: 0.0040\n",
      "Epoch [1046/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1046/3000], Train Loss: 0.0040\n",
      "Epoch [1047/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1047/3000], Train Loss: 0.0040\n",
      "Epoch [1048/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1048/3000], Train Loss: 0.0039\n",
      "Epoch [1049/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1049/3000], Train Loss: 0.0040\n",
      "Epoch [1050/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1050/3000], Train Loss: 0.0039\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1051/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1051/3000], Train Loss: 0.0039\n",
      "Epoch [1052/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1052/3000], Train Loss: 0.0039\n",
      "Epoch [1053/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1053/3000], Train Loss: 0.0039\n",
      "Epoch [1054/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1054/3000], Train Loss: 0.0039\n",
      "Epoch [1055/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1055/3000], Train Loss: 0.0039\n",
      "Epoch [1056/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1056/3000], Train Loss: 0.0039\n",
      "Epoch [1057/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1057/3000], Train Loss: 0.0039\n",
      "Epoch [1058/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1058/3000], Train Loss: 0.0039\n",
      "Epoch [1059/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1059/3000], Train Loss: 0.0038\n",
      "Epoch [1060/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1060/3000], Train Loss: 0.0038\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1061/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1061/3000], Train Loss: 0.0038\n",
      "Epoch [1062/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1062/3000], Train Loss: 0.0038\n",
      "Epoch [1063/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1063/3000], Train Loss: 0.0038\n",
      "Epoch [1064/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1064/3000], Train Loss: 0.0038\n",
      "Epoch [1065/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1065/3000], Train Loss: 0.0038\n",
      "Epoch [1066/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1066/3000], Train Loss: 0.0038\n",
      "Epoch [1067/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1067/3000], Train Loss: 0.0038\n",
      "Epoch [1068/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1068/3000], Train Loss: 0.0038\n",
      "Epoch [1069/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1069/3000], Train Loss: 0.0037\n",
      "Epoch [1070/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1070/3000], Train Loss: 0.0037\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1071/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1071/3000], Train Loss: 0.0037\n",
      "Epoch [1072/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1072/3000], Train Loss: 0.0037\n",
      "Epoch [1073/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1073/3000], Train Loss: 0.0037\n",
      "Epoch [1074/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1074/3000], Train Loss: 0.0037\n",
      "Epoch [1075/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1075/3000], Train Loss: 0.0037\n",
      "Epoch [1076/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1076/3000], Train Loss: 0.0037\n",
      "Epoch [1077/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1077/3000], Train Loss: 0.0037\n",
      "Epoch [1078/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1078/3000], Train Loss: 0.0036\n",
      "Epoch [1079/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1079/3000], Train Loss: 0.0036\n",
      "Epoch [1080/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1080/3000], Train Loss: 0.0036\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1081/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1081/3000], Train Loss: 0.0036\n",
      "Epoch [1082/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1082/3000], Train Loss: 0.0036\n",
      "Epoch [1083/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1083/3000], Train Loss: 0.0036\n",
      "Epoch [1084/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1084/3000], Train Loss: 0.0036\n",
      "Epoch [1085/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1085/3000], Train Loss: 0.0036\n",
      "Epoch [1086/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1086/3000], Train Loss: 0.0036\n",
      "Epoch [1087/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1087/3000], Train Loss: 0.0036\n",
      "Epoch [1088/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1088/3000], Train Loss: 0.0035\n",
      "Epoch [1089/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1089/3000], Train Loss: 0.0035\n",
      "Epoch [1090/3000],jacquard_loss:0.9959677419354839, hammingloss: 0.0008, Precision: 0.9960, Recall: 1.0000, F1: 0.9980\n",
      "Epoch [1090/3000], Train Loss: 0.0036\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1091/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1091/3000], Train Loss: 0.0035\n",
      "Epoch [1092/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1092/3000], Train Loss: 0.0035\n",
      "Epoch [1093/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1093/3000], Train Loss: 0.0035\n",
      "Epoch [1094/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1094/3000], Train Loss: 0.0035\n",
      "Epoch [1095/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1095/3000], Train Loss: 0.0035\n",
      "Epoch [1096/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1096/3000], Train Loss: 0.0035\n",
      "Epoch [1097/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1097/3000], Train Loss: 0.0035\n",
      "Epoch [1098/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1098/3000], Train Loss: 0.0035\n",
      "Epoch [1099/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1099/3000], Train Loss: 0.0034\n",
      "Epoch [1100/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1100/3000], Train Loss: 0.0034\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1101/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1101/3000], Train Loss: 0.0034\n",
      "Epoch [1102/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1102/3000], Train Loss: 0.0035\n",
      "Epoch [1103/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1103/3000], Train Loss: 0.0034\n",
      "Epoch [1104/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1104/3000], Train Loss: 0.0034\n",
      "Epoch [1105/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1105/3000], Train Loss: 0.0034\n",
      "Epoch [1106/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1106/3000], Train Loss: 0.0034\n",
      "Epoch [1107/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1107/3000], Train Loss: 0.0034\n",
      "Epoch [1108/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1108/3000], Train Loss: 0.0034\n",
      "Epoch [1109/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1109/3000], Train Loss: 0.0034\n",
      "Epoch [1110/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1110/3000], Train Loss: 0.0034\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1111/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1111/3000], Train Loss: 0.0034\n",
      "Epoch [1112/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1112/3000], Train Loss: 0.0033\n",
      "Epoch [1113/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1113/3000], Train Loss: 0.0033\n",
      "Epoch [1114/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1114/3000], Train Loss: 0.0033\n",
      "Epoch [1115/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1115/3000], Train Loss: 0.0033\n",
      "Epoch [1116/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1116/3000], Train Loss: 0.0033\n",
      "Epoch [1117/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1117/3000], Train Loss: 0.0033\n",
      "Epoch [1118/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1118/3000], Train Loss: 0.0033\n",
      "Epoch [1119/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1119/3000], Train Loss: 0.0033\n",
      "Epoch [1120/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1120/3000], Train Loss: 0.0033\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1121/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1121/3000], Train Loss: 0.0033\n",
      "Epoch [1122/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1122/3000], Train Loss: 0.0033\n",
      "Epoch [1123/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1123/3000], Train Loss: 0.0033\n",
      "Epoch [1124/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1124/3000], Train Loss: 0.0033\n",
      "Epoch [1125/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1125/3000], Train Loss: 0.0033\n",
      "Epoch [1126/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1126/3000], Train Loss: 0.0033\n",
      "Epoch [1127/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1127/3000], Train Loss: 0.0033\n",
      "Epoch [1128/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1128/3000], Train Loss: 0.0032\n",
      "Epoch [1129/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1129/3000], Train Loss: 0.0032\n",
      "Epoch [1130/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1130/3000], Train Loss: 0.0032\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1131/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1131/3000], Train Loss: 0.0032\n",
      "Epoch [1132/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1132/3000], Train Loss: 0.0032\n",
      "Epoch [1133/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1133/3000], Train Loss: 0.0032\n",
      "Epoch [1134/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1134/3000], Train Loss: 0.0032\n",
      "Epoch [1135/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1135/3000], Train Loss: 0.0032\n",
      "Epoch [1136/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1136/3000], Train Loss: 0.0032\n",
      "Epoch [1137/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1137/3000], Train Loss: 0.0032\n",
      "Epoch [1138/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1138/3000], Train Loss: 0.0032\n",
      "Epoch [1139/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1139/3000], Train Loss: 0.0032\n",
      "Epoch [1140/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1140/3000], Train Loss: 0.0032\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1141/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1141/3000], Train Loss: 0.0031\n",
      "Epoch [1142/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1142/3000], Train Loss: 0.0031\n",
      "Epoch [1143/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1143/3000], Train Loss: 0.0031\n",
      "Epoch [1144/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1144/3000], Train Loss: 0.0031\n",
      "Epoch [1145/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1145/3000], Train Loss: 0.0031\n",
      "Epoch [1146/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1146/3000], Train Loss: 0.0031\n",
      "Epoch [1147/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1147/3000], Train Loss: 0.0031\n",
      "Epoch [1148/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1148/3000], Train Loss: 0.0031\n",
      "Epoch [1149/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1149/3000], Train Loss: 0.0031\n",
      "Epoch [1150/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1150/3000], Train Loss: 0.0031\n",
      "Saved model to: binary_model.pth\n",
      "Epoch [1151/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1151/3000], Train Loss: 0.0031\n",
      "Epoch [1152/3000],jacquard_loss:1.0, hammingloss: 0.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch [1152/3000], Train Loss: 0.0031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# set the wandb project where this run will be logged\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4x4_matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m     17\u001b[0m     })\n\u001b[0;32m---> 18\u001b[0m train_epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, scheduler, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     35\u001b[0m gradient_norm \u001b[38;5;241m=\u001b[39m compute_gradient_norms(model)\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Step the scheduler\u001b[39;00m\n\u001b[1;32m     39\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/Mass_Spec/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Mass_Spec/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Mass_Spec/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/Mass_Spec/lib/python3.11/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/Mass_Spec/lib/python3.11/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Mass_Spec/lib/python3.11/site-packages/torch/optim/adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 394\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"4x4_matrices\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"ANN\",\n",
    "    \"dataset\": \"small_matrices.pkl\",\n",
    "    \"batch_size\": 32,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"BCELoss\",\n",
    "    \"scheduler\": \"CyclicLR\",\n",
    "    \"max_lr\": 1e-5,\n",
    "    \"base_lr\": 1e-3,\n",
    "    })\n",
    "train_epoch_losses = train(model, train_loader, optimizer, scheduler, criterion, num_epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011\n",
      "Loss: 3.6465\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            X, y = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            val_loss /= len(loader)\n",
    "    \n",
    "    print(f\"Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "y_trueTrain, y_predTrain = evaluate(model, train_loader, criterion)\n",
    "y_trueTest, y_predTest = evaluate(model, test_loader, criterion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/fvfmdq_j709g1tthj47t5fcm0000gn/T/ipykernel_74014/1668317436.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1712608632396/work/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  y_pred_train_reshaped = torch.tensor(y_predTrain).reshape(-1, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_train_reshaped = torch.tensor(y_predTrain).reshape(-1, 4, 4)\n",
    "y_pred_test_reshaped = torch.tensor(y_predTest).reshape(-1, 4, 4)\n",
    "\n",
    "y_true_train_reshaped = torch.tensor(y_trueTrain).reshape(-1, 4, 4)\n",
    "y_true_test_reshaped = torch.tensor(y_trueTest).reshape(-1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_images = 20  # Number of images to plot\n",
    "\n",
    "# Create a figure with appropriate size\n",
    "fig, axs = plt.subplots(nrows=2, ncols=num_images, figsize=(20, 4))\n",
    "\n",
    "# Iterate over the images\n",
    "for i in range(num_images):\n",
    "    # Plot predicted matrix\n",
    "    axs[0, i].imshow(y_pred_train_reshaped[i], cmap='viridis')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[0, i].set_title('Predicted', fontsize=12)\n",
    "\n",
    "    # Plot true matrix\n",
    "    axs[1, i].imshow(y_true_train_reshaped[i], cmap='viridis')\n",
    "    axs[1, i].axis('off')\n",
    "    axs[1, i].set_title('True', fontsize=12)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mass_Spec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
